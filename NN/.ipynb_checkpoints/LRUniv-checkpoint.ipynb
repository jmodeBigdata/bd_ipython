{
 "metadata": {
  "name": "",
  "signature": "sha256:c07041f5a7ea5488e6abf5f31975a1fa0f9914927bc132c0905d3d88a8aa0b62"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# coding: utf-8\n",
      "import pandas as pd\n",
      "temp=pd.read_csv(\"Univ2.csv\",header=None)\n",
      "temptarget = pd.read_csv(\"Univ3.csv\",header=None)\n",
      "\n",
      "\n",
      "#from __future__ import print_function\n",
      "import numpy as np\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "import six\n",
      "import sys\n",
      "import chainer\n",
      "import chainer.links as L\n",
      "from chainer import optimizers\n",
      "import chainer.functions as F\n",
      "from matplotlib import pyplot\n",
      "#%matplotlib inline\n",
      "\n",
      "\n",
      "batchsize = 40\n",
      "n_epoch = 20\n",
      "\n",
      "# Prepare dataset\n",
      "target_list=[]\n",
      "for i in zip(temptarget[0]):\n",
      "    target_list.append(i)\n",
      "target = np.array(target_list)\n",
      "\n",
      "feature_lists = []\n",
      "for i,j in zip(temp[0],temp[1]):\n",
      "    feature_lists.append([i,j])\n",
      "features = np.array(feature_lists)\n",
      "\n",
      "target=target.astype(np.float32).reshape(len(target_list), 1)\n",
      "feature=features.astype(np.float32)\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(feature, target, test_size=0.15)\n",
      "N_test = y_test.size  # test data size\n",
      "N = len(x_train)  # train data size\n",
      "in_units = x_train.shape[1]  \n",
      "\n",
      "\n",
      "n_units_2 = 90\n",
      "n_units_3 = 50\n",
      "n_units_4 = 30\n",
      "n_units_5 = 10\n",
      "\n",
      "model = chainer.Chain(\n",
      "    l1=L.Linear(in_units, n_units_2),\n",
      "    l2=L.Linear(n_units_2, n_units_3),\n",
      "    l3=L.Linear(n_units_3,  n_units_4),\n",
      "    l4=L.Linear(n_units_4,  n_units_5),\n",
      "    l5=L.Linear(n_units_5,  1))\n",
      "\n",
      "\n",
      "def forward(x):\n",
      "    h1 = F.relu(model.l1(x))\n",
      "    h2 = F.relu(model.l2(h1))\n",
      "    h3 = F.relu(model.l3(h2))\n",
      "    h4 = F.relu(model.l4(h3))\n",
      "    return F.tanh(model.l5(h4))\n",
      "\n",
      "# Setup optimizer\u6700\u9069\u5316\u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\n",
      "optimizer = optimizers.Adam()#\u52fe\u914d\u6cd5\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0:Adam\n",
      "optimizer.setup(model)\n",
      "#optimizer.add_hokk(optimizer.WeightDecay())#\u6b63\u5247\u5316\u3092hook\u95a2\u6570\u3068\u3057\u3066\u767b\u9332\n",
      "\n",
      "# Learning loop\n",
      "LOSS = []\n",
      "for epoch in six.moves.range(1, n_epoch + 1):\n",
      "    print('epoch', epoch)\n",
      "    # training\n",
      "    perm = np.random.permutation(N)\n",
      "    sum_loss = 0.0\n",
      "    for i in six.moves.range(0, N, batchsize):\n",
      "        x = chainer.Variable(np.asarray(x_train[perm[i:i + batchsize]]))\n",
      "        t = chainer.Variable(np.asarray(y_train[perm[i:i + batchsize]]))\n",
      "        model.zerograds()#\u52fe\u914d\u3092\u30bc\u30ed\u521d\u671f\u5316\n",
      "        y = forward(x)\n",
      "        loss = F.mean_squared_error(y, t)#\u5e73\u57472\u4e57\u8aa4\u5dee\n",
      "        sum_loss += loss.data\n",
      "        loss.backward()#\u52fe\u914d\u8a08\u7b97\n",
      "        optimizer.update()#\u6700\u9069\u5316\u30eb\u30fc\u30c1\u30f3\u3092\u5b9f\u884c\n",
      "    print('train mean loss={}'.format(sum_loss / N))\n",
      "\n",
      "    # all test data\n",
      "    x = chainer.Variable(np.asarray(x_test))\n",
      "    t = chainer.Variable(np.asarray(y_test))\n",
      "    y = forward(x)\n",
      "    loss = F.mean_squared_error(y, t)\n",
      "    LOSS.append(loss.data/N_test)\n",
      "    print(' test mean loss={}'.format(loss.data / N_test))\n",
      "print(LOSS)\n",
      "pyplot.plot(LOSS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch 1\n",
        "train mean loss=9408.646599264706"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " test mean loss=6224.340104166667\n",
        "epoch 2\n",
        "train mean loss=9424.615716911765\n",
        " test mean loss=6222.210416666667\n",
        "epoch 3\n",
        "train mean loss=9391.18262867647"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " test mean loss=6221.893229166667\n",
        "epoch 4\n",
        "train mean loss=9360.640257352941\n",
        " test mean loss=6221.824479166667\n",
        "epoch 5\n",
        "train mean loss=9353.346599264705"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " test mean loss=6221.802083333333\n",
        "epoch 6\n",
        "train mean loss=9332.221139705882\n",
        " test mean loss=6221.793229166667\n",
        "epoch 7\n",
        "train mean loss=9413.926102941177"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " test mean loss=6221.788020833334\n",
        "epoch 8\n",
        "train mean loss=9365.354779411764\n",
        " test mean loss=6221.784375\n",
        "epoch 9\n",
        "train mean loss=9315.418290441177"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " test mean loss=6221.781770833333\n",
        "epoch 10\n",
        "train mean loss=9401.935294117648\n",
        " test mean loss=6221.7796875\n",
        "epoch 11\n",
        "train mean loss=9425.918290441177"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " test mean loss=6221.777604166667\n",
        "epoch 12\n",
        "train mean loss=9429.948621323529\n",
        " test mean loss=6221.7765625\n",
        "epoch 13\n",
        "train mean loss=9441.817738970589"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " test mean loss=6221.776041666667\n",
        "epoch 14\n",
        "train mean loss=9427.3640625\n",
        " test mean loss=6221.775520833334\n",
        "epoch 15\n",
        "train mean loss=9379.771783088236"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " test mean loss=6221.774479166666\n",
        "epoch 16\n",
        "train mean loss=9404.600459558824\n",
        " test mean loss=6221.774479166666\n",
        "epoch 17\n",
        "train mean loss=9365.337959558823"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " test mean loss=6221.773958333333\n",
        "epoch 18\n",
        "train mean loss=9378.502297794117"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " test mean loss=6221.7734375\n",
        "epoch 19\n",
        "train mean loss=9371.246323529413\n",
        " test mean loss=6221.7734375\n",
        "epoch 20\n",
        "train mean loss=9415.419577205883"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " test mean loss=6221.7734375\n",
        "[6224.3401041666666, 6222.2104166666668, 6221.893229166667, 6221.8244791666666, 6221.802083333333, 6221.7932291666666, 6221.7880208333336, 6221.7843750000002, 6221.7817708333332, 6221.7796875000004, 6221.7776041666666, 6221.7765625000002, 6221.776041666667, 6221.7755208333338, 6221.7744791666664, 6221.7744791666664, 6221.7739583333332, 6221.7734375, 6221.7734375, 6221.7734375]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "[<matplotlib.lines.Line2D at 0x294dd64bb70>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEGCAYAAACevtWaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAM7+a8FeBwAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x294dd605f98>"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\u81ea\u5206\u306e\u3067\u306f\u306a\u3044\u3084\u3064\u2193\n",
      "\n",
      "\n",
      "#!/usr/bin/env python\n",
      "# coding: utf-8\n",
      "import pandas as pd\n",
      "temp=pd.read_csv(\"Univ2.csv\",header=None)\n",
      "temptarget = pd.read_csv(\"Univ3.csv\",header=None)\n",
      "\n",
      "\n",
      "#from __future__ import print_function\n",
      "import numpy as np\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "import six\n",
      "import sys\n",
      "import chainer\n",
      "import chainer.links as L\n",
      "from chainer import optimizers\n",
      "import chainer.functions as F\n",
      "\n",
      "batchsize = 40\n",
      "n_epoch = 100\n",
      "\n",
      "# Prepare dataset\n",
      "target_list=[]\n",
      "for i in zip(temptarget[0]):\n",
      "    target_list.append(i)\n",
      "target = np.array(target_list)\n",
      "\n",
      "feature_lists = []\n",
      "for i,j in zip(temp[0],temp[1]):\n",
      "    feature_lists.append([i,j])\n",
      "features = np.array(feature_lists)\n",
      "\n",
      "target=target.astype(np.float32).reshape(len(target_list), 1)\n",
      "feature=features.astype(np.float32)\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(feature, target, test_size=0.15)\n",
      "N_test = y_test.size  # test data size\n",
      "N = len(x_train)  # train data size\n",
      "in_units = x_train.shape[1]  \n",
      "\n",
      "\n",
      "n_units_2 = 90\n",
      "n_units_3 = 50\n",
      "n_units_4 = 30\n",
      "n_units_5 = 10\n",
      "\n",
      "model = chainer.Chain(\n",
      "    l1=L.Linear(in_units, n_units_2),\n",
      "    l2=L.Linear(n_units_2, n_units_3),\n",
      "    l3=L.Linear(n_units_3,  n_units_4),\n",
      "    l4=L.Linear(n_units_4,  n_units_5),\n",
      "    l5=L.Linear(n_units_5,  1))\n",
      "\n",
      "\n",
      "def forward(x):\n",
      "    h1 = F.relu(model.l1(x))\n",
      "    h2 = F.relu(model.l2(h1))\n",
      "    h3 = F.relu(model.l3(h2))\n",
      "    h4 = F.relu(model.l4(h3))\n",
      "    return F.tanh(model.l5(h4))\n",
      "\n",
      "# Setup optimizer\n",
      "optimizer = optimizers.Adam()\n",
      "optimizer.setup(model)\n",
      "\n",
      "# Learning loop\n",
      "for epoch in six.moves.range(1, n_epoch + 1):\n",
      "   for epoch in range(1, n_epoch + 1):\n",
      "    print('epoch', epoch)\n",
      "    # training\n",
      "    perm = np.random.permutation(N)\n",
      "    sum_loss = 0\n",
      "    for i in range(0, N, batchsize):\n",
      "        x = chainer.Variable(np.asarray(x_train[perm[i:i + batchsize]]))\n",
      "        t = chainer.Variable(np.asarray(y_train[perm[i:i + batchsize]]))\n",
      "        optimizer.zero_grads()\n",
      "        loss, pred = forward(x, t)\n",
      "        loss.backward()\n",
      "        optimizer.update()\n",
      "        sum_loss += float(cuda.to_cpu(loss.data)) * batchsize\n",
      "    \n",
      "    print('train mean loss={}'.format(sum_loss / N))\n",
      "    sum_loss     = 0\n",
      "    preds = []\n",
      "    for i in range(0, N_test, batchsize):\n",
      "        x_batch = chainer.Variable(np.asarray(x_test[i:i+batchsize]))\n",
      "        y_batch = chainer.Variable(np.asarray(y_test[i:i+batchsize]))\n",
      "        loss,pred = forward(x_batch, y_batch, train=False)\n",
      "        preds.extend(cuda.to_cpu(pred.data))\n",
      "        sum_loss     += float(cuda.to_cpu(loss.data)) * batchsize\n",
      "    print(' test mean loss={}'.format(sum_loss / N_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch 1\n"
       ]
      },
      {
       "ename": "TypeError",
       "evalue": "forward() takes 1 positional argument but 2 were given",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-29-7c6e560f03b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: forward() takes 1 positional argument but 2 were given"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}