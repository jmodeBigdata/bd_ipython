{
 "metadata": {
  "name": "",
  "signature": "sha256:c92e3556dd4b8d80b914c96fbd7eed84fb5ca20537d6345e84ebcc89c4d941f6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# coding: utf-8\n",
      "import pandas as pd\n",
      "temp=pd.read_csv(\"Univ2.csv\",header=None)\n",
      "temptarget = pd.read_csv(\"Univ3.csv\",header=None)\n",
      "\n",
      "\n",
      "#from __future__ import print_function\n",
      "import numpy as np\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "import six\n",
      "import sys\n",
      "import chainer\n",
      "import chainer.links as L\n",
      "from chainer import optimizers\n",
      "import chainer.functions as F\n",
      "from matplotlib import pyplot\n",
      "#%matplotlib inline\n",
      "\n",
      "\n",
      "batchsize = 40\n",
      "n_epoch = 20\n",
      "\n",
      "# Prepare dataset\n",
      "target_list=[]\n",
      "for i in zip(temptarget[0]):\n",
      "    target_list.append(i)\n",
      "target = np.array(target_list)\n",
      "\n",
      "feature_lists = []\n",
      "for i,j in zip(temp[0],temp[1]):\n",
      "    feature_lists.append([i,j])\n",
      "features = np.array(feature_lists)\n",
      "\n",
      "target=target.astype(np.float32).reshape(len(target_list), 1)\n",
      "feature=features.astype(np.float32)\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(feature, target, test_size=0.15)\n",
      "N_test = y_test.size  # test data size\n",
      "N = len(x_train)  # train data size\n",
      "in_units = x_train.shape[1]  \n",
      "\n",
      "\n",
      "n_units_2 = 90\n",
      "n_units_3 = 50\n",
      "n_units_4 = 30\n",
      "n_units_5 = 10\n",
      "\n",
      "model = chainer.Chain(\n",
      "    l1=L.Linear(in_units, n_units_2),\n",
      "    l2=L.Linear(n_units_2, n_units_3),\n",
      "    l3=L.Linear(n_units_3,  n_units_4),\n",
      "    l4=L.Linear(n_units_4,  n_units_5),\n",
      "    l5=L.Linear(n_units_5,  1))\n",
      "\n",
      "'''\n",
      "def forward(x):\n",
      "    h1 = F.relu(model.l1(x))\n",
      "    h2 = F.relu(model.l2(h1))\n",
      "    h3 = F.relu(model.l3(h2))\n",
      "    h4 = F.relu(model.l4(h3))\n",
      "    return F.tanh(model.l5(h4))\n",
      "'''\n",
      "\n",
      "def forward(x_data, y_data, train=True):\n",
      "        x = chainer.Variable(x_data)\n",
      "        t = chainer.Variable(y_data)\n",
      "        h1 = F.relu(model.l1(x))\n",
      "        h2 = F.relu(model.l2(h1))\n",
      "        h3 = F.relu(model.l3(h2))\n",
      "        h4 = F.relu(model.l4(h3))\n",
      "        y = F.reshape(model.l5(h4), (len(y_data), ))\n",
      "        return F.mean_squared_error(y, t), y\n",
      "\n",
      "# Setup optimizer\u6700\u9069\u5316\u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\n",
      "optimizer = optimizers.Adam()#\u52fe\u914d\u6cd5\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0:Adam\n",
      "optimizer.setup(model)\n",
      "#optimizer.add_hokk(optimizer.WeightDecay())#\u6b63\u5247\u5316\u3092hook\u95a2\u6570\u3068\u3057\u3066\u767b\u9332\n",
      "\n",
      "# Learning loop\n",
      "LOSS = []\n",
      "for epoch in six.moves.range(1, n_epoch + 1):\n",
      "    print('epoch', epoch)\n",
      "    # training\n",
      "    perm = np.random.permutation(N)\n",
      "    sum_loss = 0.0\n",
      "    for i in six.moves.range(0, N, batchsize):\n",
      "        x = chainer.Variable(np.asarray(x_train[perm[i:i + batchsize]]))\n",
      "        t = chainer.Variable(np.asarray(y_train[perm[i:i + batchsize]]))\n",
      "        model.zerograds()#\u52fe\u914d\u3092\u30bc\u30ed\u521d\u671f\u5316\n",
      "        y = forward(x,t)\n",
      "        loss = F.mean_squared_error(y, t)#\u5e73\u57472\u4e57\u8aa4\u5dee\n",
      "        sum_loss += loss.data\n",
      "        loss.backward()#\u52fe\u914d\u8a08\u7b97\n",
      "        optimizer.update()#\u6700\u9069\u5316\u30eb\u30fc\u30c1\u30f3\u3092\u5b9f\u884c\n",
      "    print('train mean loss={}'.format(sum_loss / N))\n",
      "\n",
      "    # all test data\n",
      "    x = chainer.Variable(np.asarray(x_test))\n",
      "    t = chainer.Variable(np.asarray(y_test))\n",
      "    y = forward(x,t)\n",
      "    loss = F.mean_squared_error(y, t)\n",
      "    LOSS.append(loss.data/N_test)\n",
      "    print(' test mean loss={}'.format(loss.data / N_test))\n",
      "print(LOSS)\n",
      "pyplot.plot(LOSS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch 1\n"
       ]
      },
      {
       "ename": "TypeError",
       "evalue": "numpy.ndarray or cuda.ndarray are expected.\nActual: <class 'chainer.variable.Variable'>",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-4-8bc9db61549b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzerograds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#\u52fe\u914d\u3092\u30bc\u30ed\u521d\u671f\u5316\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#\u5e73\u57472\u4e57\u8aa4\u5dee\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0msum_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-4-8bc9db61549b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(x_data, y_data, train)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Kensuke\\PythonPrac\\chainer\\variable.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, volatile, name)\u001b[0m\n\u001b[0;32m     91\u001b[0m             msg = '''numpy.ndarray or cuda.ndarray are expected.\n\u001b[0;32m     92\u001b[0m Actual: {0}'''.format(type(data))\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: numpy.ndarray or cuda.ndarray are expected.\nActual: <class 'chainer.variable.Variable'>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json, sys, glob, datetime, math\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import chainer\n",
      "from chainer import computational_graph as c\n",
      "from chainer import cuda\n",
      "import chainer.functions as F\n",
      "from chainer import optimizers\n",
      "%matplotlib inline\n",
      "\n",
      "class RegressionFNN:\n",
      "    def __init__(self, n_units=64, batchsize=100):\n",
      "        self.n_units = n_units\n",
      "        self.batchsize = batchsize\n",
      "        self.plotcount=0\n",
      "\n",
      "    def load(self, train_x, train_y):\n",
      "        if len(train_x)!=len(train_y):\n",
      "            raise ValueError\n",
      "        self.N = len(train_y)\n",
      "        self.I = 1\n",
      "        self.x_train = np.array(train_x, np.float32).reshape(len(train_x),1)\n",
      "        self.y_train = np.array(train_y, np.float32).reshape(len(train_y),)\n",
      "        #\n",
      "        self.model = chainer.FunctionSet(l1=F.Linear(self.I, self.n_units),\n",
      "                                        l2=F.Linear(self.n_units, self.n_units),\n",
      "                                        l3=F.Linear(self.n_units, self.n_units),\n",
      "                                        l4=F.Linear(self.n_units, 1))\n",
      "        #\n",
      "        self.optimizer = optimizers.Adam()\n",
      "        self.optimizer.setup(self.model.collect_parameters())\n",
      "\n",
      "\n",
      "    def forward(self, x_data, y_data, train=True):\n",
      "        x = chainer.Variable(x_data)\n",
      "        t = chainer.Variable(y_data)\n",
      "        h1 = F.relu(self.model.l1(x))\n",
      "        h2 = F.relu(self.model.l2(h1))\n",
      "        h3 = F.relu(self.model.l3(h2))\n",
      "        y = F.reshape(self.model.l4(h3), (len(y_data), ))\n",
      "        return F.mean_squared_error(y, t), y\n",
      "\n",
      "    def calc(self, n_epoch):\n",
      "        for epoch in range(n_epoch):\n",
      "            perm = np.random.permutation(self.N)\n",
      "            sum_loss = 0\n",
      "            #\n",
      "            for i in range(0, self.N, self.batchsize):\n",
      "                x_batch = self.x_train[perm[i:i + self.batchsize]]\n",
      "                y_batch = self.y_train[perm[i:i + self.batchsize]]\n",
      "                #\n",
      "                self.optimizer.zero_grads()\n",
      "                loss, y = self.forward(x_batch, y_batch)\n",
      "                loss.backward()\n",
      "                self.optimizer.update()\n",
      "                #\n",
      "                sum_loss += float(loss.data) * len(y_batch)\n",
      "            #  \n",
      "            print('epoch = {}, train mean loss={}\\r'.format(epoch, sum_loss / self.N), end=\"\")\n",
      "\n",
      "    def getY(self, test_x, test_y):\n",
      "        if len(test_x)!=len(test_y):\n",
      "            raise ValueError\n",
      "        x_test = np.array(test_x, np.float32).reshape(len(test_x),1)\n",
      "        y_test = np.array(test_y, np.float32).reshape(len(test_y),)\n",
      "        loss, y = self.forward(x_test, y_test, train=False)\n",
      "        '''\n",
      "        with open(\"output/{}.csv\".format(self.plotcount), \"w\") as f:\n",
      "            f.write(\"\\n\".join([\"{},{}\".format(ux,uy) for ux,uy in zip(y.data, y_test)]))\n",
      "\n",
      "        #'''\n",
      "        plt.clf()\n",
      "        plt.plot(x_test, y_test, color=\"b\", label=\"original\")\n",
      "        plt.plot(x_test, y.data, color=\"g\", label=\"RegFNN\")\n",
      "        plt.legend(loc = 'lower left')\n",
      "        #plt.ylim(-1.2,1.2)\n",
      "        plt.grid(True)\n",
      "        '''\n",
      "        plt.savefig(\"output/{}.png\".format(self.plotcount))\n",
      "        self.plotcount+=1\n",
      "        '''\n",
      "if __name__==\"__main__\":\n",
      "    rf = RegressionFNN(n_units=64, batchsize=400)\n",
      "    rf.load(x_train,y_train)\n",
      "    rf.calc(500) # epoch\n",
      "    rf.getY((3.4,1))\n",
      "    print(\"\\ndone.\")\n",
      "    plt.show()\n",
      "plt.plot(d)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "total size of new array must be unchanged",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-17-a3d00a932ac1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegressionFNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetY\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3.4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-17-a3d00a932ac1>\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, train_x, train_y)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: total size of new array must be unchanged"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "temp=pd.read_csv(\"Univ2.csv\",header=None)\n",
      "temptarget = pd.read_csv(\"Univ3.csv\",header=None)\n",
      "import numpy as np\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "import six\n",
      "import sys\n",
      "import chainer\n",
      "import chainer.links as L\n",
      "from chainer import optimizers\n",
      "import chainer.functions as F\n",
      "from matplotlib import pyplot\n",
      "\n",
      "# Prepare dataset\n",
      "target_list=[]\n",
      "for i in zip(temptarget[0]):\n",
      "    target_list.append(i)\n",
      "target = np.array(target_list)\n",
      "\n",
      "feature_lists = []\n",
      "for i,j in zip(temp[0],temp[1]):\n",
      "    feature_lists.append([i,j])\n",
      "features = np.array(feature_lists)\n",
      "\n",
      "target=target.astype(np.float32).reshape(len(target_list), 1)\n",
      "feature=features.astype(np.float32)\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(feature, target, test_size=0.15)\n",
      "N_test = y_test.size  # test data size\n",
      "N = len(x_train)  # train data size\n",
      "in_units = x_train.shape[1]  \n",
      "\n",
      "print(x_train,y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 3.3499999   3.        ]\n",
        " [ 3.83999991  3.        ]\n",
        " [ 3.50999999  2.        ]\n",
        " [ 3.01999998  1.        ]\n",
        " [ 3.1500001   2.        ]\n",
        " [ 3.30999994  3.        ]\n",
        " [ 3.20000005  1.        ]\n",
        " [ 3.8599999   2.        ]\n",
        " [ 4.          1.        ]\n",
        " [ 2.80999994  3.        ]\n",
        " [ 3.23000002  4.        ]\n",
        " [ 3.5999999   3.        ]\n",
        " [ 3.00999999  3.        ]\n",
        " [ 3.45000005  4.        ]\n",
        " [ 3.45000005  2.        ]\n",
        " [ 2.93000007  2.        ]\n",
        " [ 2.69000006  2.        ]\n",
        " [ 2.54999995  1.        ]\n",
        " [ 3.57999992  2.        ]\n",
        " [ 4.          2.        ]\n",
        " [ 4.          4.        ]\n",
        " [ 3.04999995  2.        ]\n",
        " [ 2.77999997  3.        ]\n",
        " [ 4.          3.        ]\n",
        " [ 3.79999995  2.        ]\n",
        " [ 3.1400001   3.        ]\n",
        " [ 3.74000001  4.        ]\n",
        " [ 3.36999989  2.        ]\n",
        " [ 3.49000001  2.        ]\n",
        " [ 2.55999994  3.        ]\n",
        " [ 3.6400001   1.        ]\n",
        " [ 2.80999994  1.        ]\n",
        " [ 3.45000005  2.        ]\n",
        " [ 3.79999995  3.        ]\n",
        " [ 3.11999989  3.        ]\n",
        " [ 3.80999994  1.        ]\n",
        " [ 4.          3.        ]\n",
        " [ 3.26999998  2.        ]\n",
        " [ 4.          1.        ]\n",
        " [ 3.07999992  2.        ]\n",
        " [ 3.50999999  3.        ]\n",
        " [ 3.58999991  4.        ]\n",
        " [ 3.33999991  4.        ]\n",
        " [ 3.43000007  3.        ]\n",
        " [ 3.50999999  2.        ]\n",
        " [ 2.80999994  3.        ]\n",
        " [ 3.4000001   3.        ]\n",
        " [ 3.56999993  3.        ]\n",
        " [ 3.80999994  1.        ]\n",
        " [ 3.63000011  2.        ]\n",
        " [ 4.          3.        ]\n",
        " [ 3.08999991  4.        ]\n",
        " [ 3.4000001   4.        ]\n",
        " [ 2.71000004  3.        ]\n",
        " [ 3.28999996  1.        ]\n",
        " [ 2.78999996  2.        ]\n",
        " [ 2.93000007  3.        ]\n",
        " [ 2.91000009  3.        ]\n",
        " [ 3.22000003  2.        ]\n",
        " [ 3.76999998  3.        ]\n",
        " [ 3.73000002  3.        ]\n",
        " [ 3.69000006  1.        ]\n",
        " [ 3.48000002  2.        ]\n",
        " [ 3.61999989  4.        ]\n",
        " [ 3.24000001  4.        ]\n",
        " [ 3.47000003  3.        ]\n",
        " [ 3.48000002  2.        ]\n",
        " [ 2.77999997  2.        ]\n",
        " [ 3.94000006  4.        ]\n",
        " [ 3.6400001   3.        ]\n",
        " [ 2.98000002  1.        ]\n",
        " [ 3.27999997  1.        ]\n",
        " [ 3.76999998  3.        ]\n",
        " [ 3.13000011  4.        ]\n",
        " [ 3.92000008  4.        ]\n",
        " [ 2.98000002  2.        ]\n",
        " [ 2.67000008  2.        ]\n",
        " [ 3.6500001   4.        ]\n",
        " [ 4.          2.        ]\n",
        " [ 2.83999991  2.        ]\n",
        " [ 3.17000008  1.        ]\n",
        " [ 3.6099999   3.        ]\n",
        " [ 3.74000001  2.        ]\n",
        " [ 2.8499999   3.        ]\n",
        " [ 3.33999991  2.        ]\n",
        " [ 3.32999992  4.        ]\n",
        " [ 3.94000006  3.        ]\n",
        " [ 2.70000005  2.        ]\n",
        " [ 3.47000003  2.        ]\n",
        " [ 3.24000001  4.        ]\n",
        " [ 3.88000011  3.        ]\n",
        " [ 3.07999992  3.        ]\n",
        " [ 3.36999989  4.        ]\n",
        " [ 3.38000011  4.        ]\n",
        " [ 3.72000003  2.        ]\n",
        " [ 2.68000007  3.        ]\n",
        " [ 3.44000006  3.        ]\n",
        " [ 3.31999993  1.        ]\n",
        " [ 3.3900001   3.        ]\n",
        " [ 2.82999992  3.        ]\n",
        " [ 3.01999998  1.        ]\n",
        " [ 2.98000002  3.        ]\n",
        " [ 3.63000011  3.        ]\n",
        " [ 3.73000002  1.        ]\n",
        " [ 3.70000005  2.        ]\n",
        " [ 3.6500001   2.        ]\n",
        " [ 3.00999999  4.        ]\n",
        " [ 3.29999995  2.        ]\n",
        " [ 3.49000001  4.        ]\n",
        " [ 3.83999991  2.        ]\n",
        " [ 4.          1.        ]\n",
        " [ 3.32999992  2.        ]\n",
        " [ 3.92000008  2.        ]\n",
        " [ 3.6099999   1.        ]\n",
        " [ 3.76999998  2.        ]\n",
        " [ 3.3599999   3.        ]\n",
        " [ 3.56999993  3.        ]\n",
        " [ 3.66000009  1.        ]\n",
        " [ 2.97000003  2.        ]\n",
        " [ 3.13000011  2.        ]\n",
        " [ 3.67000008  2.        ]\n",
        " [ 3.46000004  4.        ]\n",
        " [ 3.03999996  3.        ]\n",
        " [ 3.9000001   2.        ]\n",
        " [ 3.6400001   3.        ]\n",
        " [ 3.93000007  2.        ]\n",
        " [ 3.41000009  4.        ]\n",
        " [ 3.19000006  2.        ]\n",
        " [ 3.53999996  1.        ]\n",
        " [ 3.95000005  3.        ]\n",
        " [ 3.75        2.        ]\n",
        " [ 3.4000001   3.        ]\n",
        " [ 4.          3.        ]\n",
        " [ 3.30999994  1.        ]\n",
        " [ 3.53999996  3.        ]\n",
        " [ 3.46000004  2.        ]\n",
        " [ 4.          3.        ]\n",
        " [ 2.51999998  2.        ]\n",
        " [ 2.75999999  2.        ]\n",
        " [ 2.93000007  4.        ]\n",
        " [ 2.94000006  2.        ]\n",
        " [ 3.28999996  4.        ]\n",
        " [ 4.          2.        ]\n",
        " [ 3.38000011  2.        ]\n",
        " [ 2.48000002  4.        ]\n",
        " [ 2.88000011  2.        ]\n",
        " [ 3.70000005  1.        ]\n",
        " [ 2.81999993  4.        ]\n",
        " [ 3.8900001   1.        ]\n",
        " [ 3.          3.        ]\n",
        " [ 2.96000004  3.        ]\n",
        " [ 3.1500001   3.        ]\n",
        " [ 3.1400001   2.        ]\n",
        " [ 3.48000002  3.        ]\n",
        " [ 2.98000002  1.        ]\n",
        " [ 3.43000007  2.        ]\n",
        " [ 3.57999992  1.        ]\n",
        " [ 2.92000008  4.        ]\n",
        " [ 3.33999991  3.        ]\n",
        " [ 3.77999997  4.        ]\n",
        " [ 3.45000005  4.        ]\n",
        " [ 3.67000008  2.        ]\n",
        " [ 3.50999999  2.        ]\n",
        " [ 2.71000004  2.        ]\n",
        " [ 2.25999999  4.        ]\n",
        " [ 3.19000006  3.        ]\n",
        " [ 2.81999993  4.        ]\n",
        " [ 3.3499999   3.        ]\n",
        " [ 4.          3.        ]\n",
        " [ 3.03999996  1.        ]\n",
        " [ 3.          2.        ]\n",
        " [ 3.3599999   1.        ]\n",
        " [ 4.          1.        ]\n",
        " [ 3.63000011  3.        ]\n",
        " [ 3.67000008  3.        ]\n",
        " [ 3.3900001   4.        ]\n",
        " [ 3.01999998  4.        ]\n",
        " [ 3.31999993  4.        ]\n",
        " [ 3.71000004  1.        ]\n",
        " [ 2.96000004  1.        ]\n",
        " [ 3.06999993  2.        ]\n",
        " [ 3.22000003  1.        ]\n",
        " [ 3.21000004  4.        ]\n",
        " [ 3.30999994  3.        ]\n",
        " [ 3.3499999   2.        ]\n",
        " [ 4.          2.        ]\n",
        " [ 3.17000008  2.        ]\n",
        " [ 3.19000006  4.        ]\n",
        " [ 3.97000003  1.        ]\n",
        " [ 2.91000009  1.        ]\n",
        " [ 3.3499999   2.        ]\n",
        " [ 3.20000005  2.        ]\n",
        " [ 3.71000004  4.        ]\n",
        " [ 3.9000001   1.        ]\n",
        " [ 3.06999993  2.        ]\n",
        " [ 3.6500001   2.        ]\n",
        " [ 3.38000011  3.        ]\n",
        " [ 3.95000005  2.        ]\n",
        " [ 3.50999999  2.        ]\n",
        " [ 3.1500001   4.        ]\n",
        " [ 3.3499999   2.        ]\n",
        " [ 3.27999997  3.        ]\n",
        " [ 3.61999989  3.        ]\n",
        " [ 2.63000011  2.        ]\n",
        " [ 3.58999991  2.        ]\n",
        " [ 3.07999992  4.        ]\n",
        " [ 3.30999994  3.        ]\n",
        " [ 3.38000011  2.        ]\n",
        " [ 3.5         2.        ]\n",
        " [ 3.95000005  3.        ]\n",
        " [ 3.05999994  2.        ]\n",
        " [ 3.51999998  4.        ]\n",
        " [ 3.31999993  2.        ]\n",
        " [ 2.67000008  3.        ]\n",
        " [ 3.1500001   4.        ]\n",
        " [ 3.38000011  3.        ]\n",
        " [ 3.6400001   3.        ]\n",
        " [ 3.1400001   2.        ]\n",
        " [ 3.5         2.        ]\n",
        " [ 3.33999991  3.        ]\n",
        " [ 3.8900001   3.        ]\n",
        " [ 2.70000005  3.        ]\n",
        " [ 2.94000006  2.        ]\n",
        " [ 3.74000001  1.        ]\n",
        " [ 3.76999998  4.        ]\n",
        " [ 3.69000006  3.        ]\n",
        " [ 3.06999993  2.        ]\n",
        " [ 3.99000001  3.        ]\n",
        " [ 2.6500001   3.        ]\n",
        " [ 3.29999995  1.        ]\n",
        " [ 2.93000007  4.        ]\n",
        " [ 3.13000011  2.        ]\n",
        " [ 3.6099999   3.        ]\n",
        " [ 3.56999993  2.        ]\n",
        " [ 3.16000009  1.        ]\n",
        " [ 3.3599999   2.        ]\n",
        " [ 2.9000001   4.        ]\n",
        " [ 3.95000005  4.        ]\n",
        " [ 3.77999997  2.        ]\n",
        " [ 3.43000007  2.        ]\n",
        " [ 3.25        3.        ]\n",
        " [ 2.73000002  2.        ]\n",
        " [ 3.69000006  3.        ]\n",
        " [ 3.29999995  2.        ]\n",
        " [ 3.91000009  3.        ]\n",
        " [ 3.22000003  1.        ]\n",
        " [ 3.4000001   2.        ]\n",
        " [ 3.94000006  3.        ]\n",
        " [ 2.9000001   1.        ]\n",
        " [ 3.44000006  2.        ]\n",
        " [ 3.5999999   3.        ]\n",
        " [ 3.4000001   2.        ]\n",
        " [ 3.06999993  3.        ]\n",
        " [ 3.30999994  4.        ]\n",
        " [ 3.26999998  2.        ]\n",
        " [ 4.          2.        ]\n",
        " [ 2.86999989  2.        ]\n",
        " [ 3.95000005  2.        ]\n",
        " [ 3.49000001  1.        ]\n",
        " [ 3.30999994  1.        ]\n",
        " [ 3.70000005  4.        ]\n",
        " [ 3.58999991  2.        ]\n",
        " [ 2.9000001   3.        ]\n",
        " [ 3.01999998  2.        ]\n",
        " [ 3.1099999   2.        ]\n",
        " [ 3.76999998  3.        ]\n",
        " [ 2.9000001   2.        ]\n",
        " [ 3.33999991  2.        ]\n",
        " [ 3.63000011  1.        ]\n",
        " [ 2.42000008  1.        ]\n",
        " [ 3.13000011  2.        ]\n",
        " [ 3.43000007  3.        ]\n",
        " [ 3.27999997  1.        ]\n",
        " [ 4.          3.        ]\n",
        " [ 3.3499999   3.        ]\n",
        " [ 3.94000006  2.        ]\n",
        " [ 3.4000001   2.        ]\n",
        " [ 3.02999997  3.        ]\n",
        " [ 2.97000003  4.        ]\n",
        " [ 3.8900001   2.        ]\n",
        " [ 3.22000003  1.        ]\n",
        " [ 3.0999999   4.        ]\n",
        " [ 2.8599999   4.        ]\n",
        " [ 3.67000008  3.        ]\n",
        " [ 3.81999993  3.        ]\n",
        " [ 3.99000001  3.        ]\n",
        " [ 3.54999995  4.        ]\n",
        " [ 3.52999997  1.        ]\n",
        " [ 2.61999989  2.        ]\n",
        " [ 2.93000007  3.        ]\n",
        " [ 3.63000011  2.        ]\n",
        " [ 3.55999994  1.        ]\n",
        " [ 3.43000007  3.        ]\n",
        " [ 3.57999992  1.        ]\n",
        " [ 3.94000006  2.        ]\n",
        " [ 3.11999989  1.        ]\n",
        " [ 3.04999995  2.        ]\n",
        " [ 2.42000008  2.        ]\n",
        " [ 3.19000006  4.        ]\n",
        " [ 3.22000003  2.        ]\n",
        " [ 3.57999992  2.        ]\n",
        " [ 3.75        2.        ]\n",
        " [ 4.          1.        ]\n",
        " [ 3.          4.        ]\n",
        " [ 2.78999996  4.        ]\n",
        " [ 3.63000011  4.        ]\n",
        " [ 3.58999991  3.        ]\n",
        " [ 3.30999994  2.        ]\n",
        " [ 3.18000007  2.        ]\n",
        " [ 3.80999994  2.        ]\n",
        " [ 3.99000001  3.        ]\n",
        " [ 3.32999992  3.        ]\n",
        " [ 3.51999998  2.        ]\n",
        " [ 3.51999998  4.        ]\n",
        " [ 3.26999998  3.        ]\n",
        " [ 4.          3.        ]\n",
        " [ 3.46000004  2.        ]\n",
        " [ 2.94000006  3.        ]\n",
        " [ 3.17000008  2.        ]\n",
        " [ 3.27999997  3.        ]\n",
        " [ 4.          2.        ]\n",
        " [ 4.          1.        ]\n",
        " [ 3.1400001   1.        ]\n",
        " [ 3.9000001   3.        ]\n",
        " [ 4.          2.        ]\n",
        " [ 2.61999989  2.        ]\n",
        " [ 3.8599999   3.        ]\n",
        " [ 3.98000002  2.        ]\n",
        " [ 3.8499999   3.        ]\n",
        " [ 3.3900001   2.        ]\n",
        " [ 3.6400001   1.        ]\n",
        " [ 3.57999992  1.        ]\n",
        " [ 3.49000001  2.        ]\n",
        " [ 3.11999989  3.        ]\n",
        " [ 3.5999999   2.        ]\n",
        " [ 3.42000008  2.        ]\n",
        " [ 3.75999999  3.        ]\n",
        " [ 3.75999999  3.        ]\n",
        " [ 3.11999989  2.        ]\n",
        " [ 3.30999994  4.        ]] [[ 400.]\n",
        " [ 720.]\n",
        " [ 520.]\n",
        " [ 480.]\n",
        " [ 400.]\n",
        " [ 500.]\n",
        " [ 540.]\n",
        " [ 740.]\n",
        " [ 760.]\n",
        " [ 500.]\n",
        " [ 400.]\n",
        " [ 660.]\n",
        " [ 300.]\n",
        " [ 720.]\n",
        " [ 440.]\n",
        " [ 580.]\n",
        " [ 420.]\n",
        " [ 480.]\n",
        " [ 620.]\n",
        " [ 660.]\n",
        " [ 800.]\n",
        " [ 400.]\n",
        " [ 480.]\n",
        " [ 560.]\n",
        " [ 580.]\n",
        " [ 460.]\n",
        " [ 740.]\n",
        " [ 620.]\n",
        " [ 660.]\n",
        " [ 360.]\n",
        " [ 720.]\n",
        " [ 760.]\n",
        " [ 620.]\n",
        " [ 780.]\n",
        " [ 580.]\n",
        " [ 520.]\n",
        " [ 700.]\n",
        " [ 680.]\n",
        " [ 520.]\n",
        " [ 700.]\n",
        " [ 400.]\n",
        " [ 380.]\n",
        " [ 740.]\n",
        " [ 560.]\n",
        " [ 640.]\n",
        " [ 540.]\n",
        " [ 600.]\n",
        " [ 500.]\n",
        " [ 540.]\n",
        " [ 660.]\n",
        " [ 640.]\n",
        " [ 620.]\n",
        " [ 580.]\n",
        " [ 560.]\n",
        " [ 520.]\n",
        " [ 640.]\n",
        " [ 460.]\n",
        " [ 660.]\n",
        " [ 620.]\n",
        " [ 660.]\n",
        " [ 640.]\n",
        " [ 580.]\n",
        " [ 600.]\n",
        " [ 740.]\n",
        " [ 440.]\n",
        " [ 800.]\n",
        " [ 560.]\n",
        " [ 800.]\n",
        " [ 620.]\n",
        " [ 680.]\n",
        " [ 560.]\n",
        " [ 540.]\n",
        " [ 720.]\n",
        " [ 440.]\n",
        " [ 420.]\n",
        " [ 600.]\n",
        " [ 480.]\n",
        " [ 520.]\n",
        " [ 480.]\n",
        " [ 300.]\n",
        " [ 540.]\n",
        " [ 380.]\n",
        " [ 520.]\n",
        " [ 520.]\n",
        " [ 680.]\n",
        " [ 380.]\n",
        " [ 720.]\n",
        " [ 540.]\n",
        " [ 600.]\n",
        " [ 560.]\n",
        " [ 720.]\n",
        " [ 500.]\n",
        " [ 740.]\n",
        " [ 540.]\n",
        " [ 700.]\n",
        " [ 520.]\n",
        " [ 480.]\n",
        " [ 660.]\n",
        " [ 540.]\n",
        " [ 220.]\n",
        " [ 420.]\n",
        " [ 500.]\n",
        " [ 600.]\n",
        " [ 800.]\n",
        " [ 680.]\n",
        " [ 700.]\n",
        " [ 500.]\n",
        " [ 520.]\n",
        " [ 560.]\n",
        " [ 540.]\n",
        " [ 580.]\n",
        " [ 660.]\n",
        " [ 700.]\n",
        " [ 620.]\n",
        " [ 540.]\n",
        " [ 560.]\n",
        " [ 580.]\n",
        " [ 800.]\n",
        " [ 740.]\n",
        " [ 480.]\n",
        " [ 680.]\n",
        " [ 540.]\n",
        " [ 560.]\n",
        " [ 800.]\n",
        " [ 460.]\n",
        " [ 640.]\n",
        " [ 420.]\n",
        " [ 540.]\n",
        " [ 600.]\n",
        " [ 620.]\n",
        " [ 620.]\n",
        " [ 720.]\n",
        " [ 800.]\n",
        " [ 720.]\n",
        " [ 800.]\n",
        " [ 580.]\n",
        " [ 720.]\n",
        " [ 560.]\n",
        " [ 440.]\n",
        " [ 520.]\n",
        " [ 700.]\n",
        " [ 580.]\n",
        " [ 620.]\n",
        " [ 400.]\n",
        " [ 440.]\n",
        " [ 580.]\n",
        " [ 800.]\n",
        " [ 600.]\n",
        " [ 600.]\n",
        " [ 360.]\n",
        " [ 680.]\n",
        " [ 520.]\n",
        " [ 660.]\n",
        " [ 680.]\n",
        " [ 460.]\n",
        " [ 800.]\n",
        " [ 480.]\n",
        " [ 300.]\n",
        " [ 660.]\n",
        " [ 540.]\n",
        " [ 660.]\n",
        " [ 660.]\n",
        " [ 580.]\n",
        " [ 500.]\n",
        " [ 420.]\n",
        " [ 520.]\n",
        " [ 600.]\n",
        " [ 520.]\n",
        " [ 500.]\n",
        " [ 540.]\n",
        " [ 760.]\n",
        " [ 560.]\n",
        " [ 700.]\n",
        " [ 620.]\n",
        " [ 660.]\n",
        " [ 480.]\n",
        " [ 540.]\n",
        " [ 560.]\n",
        " [ 620.]\n",
        " [ 420.]\n",
        " [ 560.]\n",
        " [ 600.]\n",
        " [ 620.]\n",
        " [ 800.]\n",
        " [ 600.]\n",
        " [ 520.]\n",
        " [ 620.]\n",
        " [ 680.]\n",
        " [ 800.]\n",
        " [ 480.]\n",
        " [ 760.]\n",
        " [ 580.]\n",
        " [ 480.]\n",
        " [ 680.]\n",
        " [ 460.]\n",
        " [ 400.]\n",
        " [ 640.]\n",
        " [ 640.]\n",
        " [ 540.]\n",
        " [ 460.]\n",
        " [ 500.]\n",
        " [ 540.]\n",
        " [ 600.]\n",
        " [ 460.]\n",
        " [ 560.]\n",
        " [ 680.]\n",
        " [ 400.]\n",
        " [ 380.]\n",
        " [ 580.]\n",
        " [ 620.]\n",
        " [ 580.]\n",
        " [ 700.]\n",
        " [ 580.]\n",
        " [ 420.]\n",
        " [ 800.]\n",
        " [ 600.]\n",
        " [ 600.]\n",
        " [ 680.]\n",
        " [ 540.]\n",
        " [ 380.]\n",
        " [ 600.]\n",
        " [ 520.]\n",
        " [ 640.]\n",
        " [ 800.]\n",
        " [ 580.]\n",
        " [ 620.]\n",
        " [ 620.]\n",
        " [ 460.]\n",
        " [ 560.]\n",
        " [ 620.]\n",
        " [ 500.]\n",
        " [ 600.]\n",
        " [ 560.]\n",
        " [ 480.]\n",
        " [ 560.]\n",
        " [ 400.]\n",
        " [ 700.]\n",
        " [ 500.]\n",
        " [ 540.]\n",
        " [ 480.]\n",
        " [ 520.]\n",
        " [ 520.]\n",
        " [ 560.]\n",
        " [ 580.]\n",
        " [ 800.]\n",
        " [ 800.]\n",
        " [ 580.]\n",
        " [ 540.]\n",
        " [ 340.]\n",
        " [ 460.]\n",
        " [ 500.]\n",
        " [ 480.]\n",
        " [ 660.]\n",
        " [ 660.]\n",
        " [ 700.]\n",
        " [ 700.]\n",
        " [ 460.]\n",
        " [ 660.]\n",
        " [ 540.]\n",
        " [ 740.]\n",
        " [ 660.]\n",
        " [ 560.]\n",
        " [ 520.]\n",
        " [ 580.]\n",
        " [ 680.]\n",
        " [ 460.]\n",
        " [ 800.]\n",
        " [ 580.]\n",
        " [ 640.]\n",
        " [ 680.]\n",
        " [ 500.]\n",
        " [ 620.]\n",
        " [ 700.]\n",
        " [ 580.]\n",
        " [ 640.]\n",
        " [ 640.]\n",
        " [ 620.]\n",
        " [ 500.]\n",
        " [ 500.]\n",
        " [ 800.]\n",
        " [ 440.]\n",
        " [ 520.]\n",
        " [ 500.]\n",
        " [ 640.]\n",
        " [ 700.]\n",
        " [ 620.]\n",
        " [ 540.]\n",
        " [ 800.]\n",
        " [ 480.]\n",
        " [ 400.]\n",
        " [ 620.]\n",
        " [ 700.]\n",
        " [ 380.]\n",
        " [ 600.]\n",
        " [ 660.]\n",
        " [ 540.]\n",
        " [ 800.]\n",
        " [ 560.]\n",
        " [ 640.]\n",
        " [ 780.]\n",
        " [ 460.]\n",
        " [ 800.]\n",
        " [ 660.]\n",
        " [ 680.]\n",
        " [ 500.]\n",
        " [ 780.]\n",
        " [ 660.]\n",
        " [ 680.]\n",
        " [ 620.]\n",
        " [ 640.]\n",
        " [ 680.]\n",
        " [ 540.]\n",
        " [ 700.]\n",
        " [ 740.]\n",
        " [ 360.]\n",
        " [ 740.]\n",
        " [ 680.]\n",
        " [ 380.]\n",
        " [ 640.]\n",
        " [ 600.]\n",
        " [ 780.]\n",
        " [ 700.]\n",
        " [ 360.]\n",
        " [ 520.]\n",
        " [ 580.]\n",
        " [ 520.]\n",
        " [ 640.]\n",
        " [ 680.]\n",
        " [ 680.]\n",
        " [ 440.]\n",
        " [ 460.]\n",
        " [ 580.]\n",
        " [ 540.]\n",
        " [ 640.]\n",
        " [ 800.]\n",
        " [ 720.]\n",
        " [ 680.]\n",
        " [ 640.]\n",
        " [ 520.]\n",
        " [ 600.]]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}