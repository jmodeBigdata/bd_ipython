{
 "metadata": {
  "name": "",
  "signature": "sha256:4d1ef32882add0cb9f9298be580b34e5c979226e638ced005b92610de457e40e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json, sys, glob, datetime, math\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import chainer\n",
      "from chainer import computational_graph as c\n",
      "from chainer import cuda\n",
      "import chainer.functions as F\n",
      "from chainer import optimizers\n",
      "%matplotlib inline\n",
      "\n",
      "class RegressionFNN:\n",
      "    def __init__(self, n_units=64, batchsize=100):\n",
      "        self.n_units = n_units\n",
      "        self.batchsize = batchsize\n",
      "        self.plotcount=0\n",
      "\n",
      "    def load(self, train_x, train_y):\n",
      "        if len(train_x)!=len(train_y):\n",
      "            raise ValueError\n",
      "        self.N = len(train_y)\n",
      "        self.I = 1\n",
      "        self.x_train = np.array(train_x, np.float32).reshape(len(train_x),1)\n",
      "        self.y_train = np.array(train_y, np.float32).reshape(len(train_y),)\n",
      "        #\n",
      "        self.model = chainer.FunctionSet(l1=F.Linear(self.I, self.n_units),\n",
      "                                        l2=F.Linear(self.n_units, self.n_units),\n",
      "                                        l3=F.Linear(self.n_units, self.n_units),\n",
      "                                        l4=F.Linear(self.n_units, 1))\n",
      "        #\n",
      "        self.optimizer = optimizers.Adam()\n",
      "        self.optimizer.setup(self.model.collect_parameters())\n",
      "\n",
      "\n",
      "    def forward(self, x_data, y_data, train=True):\n",
      "        x = chainer.Variable(x_data)\n",
      "        t = chainer.Variable(y_data)\n",
      "        h1 = F.relu(self.model.l1(x))\n",
      "        h2 = F.relu(self.model.l2(h1))\n",
      "        h3 = F.relu(self.model.l3(h2))\n",
      "        y = F.reshape(self.model.l4(h3), (len(y_data), ))\n",
      "        return F.mean_squared_error(y, t), y\n",
      "\n",
      "    def calc(self, n_epoch):\n",
      "        for epoch in range(n_epoch):\n",
      "            perm = np.random.permutation(self.N)\n",
      "            sum_loss = 0\n",
      "            #\n",
      "            for i in range(0, self.N, self.batchsize):\n",
      "                x_batch = self.x_train[perm[i:i + self.batchsize]]\n",
      "                y_batch = self.y_train[perm[i:i + self.batchsize]]\n",
      "                #\n",
      "                self.optimizer.zero_grads()\n",
      "                loss, y = self.forward(x_batch, y_batch)\n",
      "                loss.backward()\n",
      "                self.optimizer.update()\n",
      "                #\n",
      "                sum_loss += float(loss.data) * len(y_batch)\n",
      "            #  \n",
      "            print('epoch = {}, train mean loss={}\\r'.format(epoch, sum_loss / self.N), end=\"\")\n",
      "\n",
      "    def getY(self, test_x, test_y):\n",
      "        if len(test_x)!=len(test_y):\n",
      "            raise ValueError\n",
      "        x_test = np.array(test_x, np.float32).reshape(len(test_x),1)\n",
      "        y_test = np.array(test_y, np.float32).reshape(len(test_y),)\n",
      "        loss, y = self.forward(x_test, y_test, train=False)\n",
      "        '''\n",
      "        with open(\"output/{}.csv\".format(self.plotcount), \"w\") as f:\n",
      "            f.write(\"\\n\".join([\"{},{}\".format(ux,uy) for ux,uy in zip(y.data, y_test)]))\n",
      "\n",
      "        #'''\n",
      "        plt.clf()\n",
      "        plt.plot(x_test, y_test, color=\"b\", label=\"original\")\n",
      "        plt.plot(x_test, y.data, color=\"g\", label=\"RegFNN\")\n",
      "        plt.legend(loc = 'lower left')\n",
      "        plt.ylim(-1.2,1.2)\n",
      "        plt.grid(True)\n",
      "        print(y.data)\n",
      "        '''\n",
      "        plt.savefig(\"output/{}.png\".format(self.plotcount))\n",
      "        self.plotcount+=1\n",
      "        '''\n",
      "\n",
      "def f(d):\n",
      "    return [math.sin(u) for u in d]\n",
      "if __name__==\"__main__\":\n",
      "    rf = RegressionFNN(n_units=64, batchsize=314)\n",
      "    d = [u*0.02 for u in range(314)]\n",
      "    rf.load(d,f(d))\n",
      "    rf.calc(800) # epoch\n",
      "    rf.getY(d,f(d))\n",
      "    print(\"\\ndone.\")\n",
      "    plt.show()\n",
      "plt.plot(d)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 0, train mean loss=0.4495459794998169\r",
        "epoch = 1, train mean loss=0.46298325061798096\r",
        "epoch = 2, train mean loss=0.43860262632369995\r",
        "epoch = 3, train mean loss=0.4136370122432709\r",
        "epoch = 4, train mean loss=0.4252995252609253\r",
        "epoch = 5, train mean loss=0.42683351039886475\r",
        "epoch = 6, train mean loss=0.4099276661872864\r",
        "epoch = 7, train mean loss=0.3997098505496979\r",
        "epoch = 8, train mean loss=0.40453967452049255\r",
        "epoch = 9, train mean loss=0.4062187969684601\r",
        "epoch = 10, train mean loss=0.395998477935791\r",
        "epoch = 11, train mean loss=0.3864423930644989\r",
        "epoch = 12, train mean loss=0.38498932123184204\r",
        "epoch = 13, train mean loss=0.3855085074901581\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 14, train mean loss=0.38101136684417725\r",
        "epoch = 15, train mean loss=0.37337175011634827\r",
        "epoch = 16, train mean loss=0.3689454197883606\r",
        "epoch = 17, train mean loss=0.36831802129745483\r",
        "epoch = 18, train mean loss=0.36637282371520996\r",
        "epoch = 19, train mean loss=0.36089685559272766\r",
        "epoch = 20, train mean loss=0.3553347587585449\r",
        "epoch = 21, train mean loss=0.3526865243911743\r",
        "epoch = 22, train mean loss=0.3510756194591522\r",
        "epoch = 23, train mean loss=0.3473496735095978\r",
        "epoch = 24, train mean loss=0.3421935439109802\r",
        "epoch = 25, train mean loss=0.33845633268356323\r",
        "epoch = 26, train mean loss=0.33630943298339844\r",
        "epoch = 27, train mean loss=0.3333146572113037\r",
        "epoch = 28, train mean loss=0.32879123091697693\r",
        "epoch = 29, train mean loss=0.3246932029724121\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 30, train mean loss=0.3219601511955261\r",
        "epoch = 31, train mean loss=0.31902939081192017\r",
        "epoch = 32, train mean loss=0.31486284732818604\r",
        "epoch = 33, train mean loss=0.31044286489486694\r",
        "epoch = 34, train mean loss=0.30747222900390625\r",
        "epoch = 35, train mean loss=0.3043810725212097\r",
        "epoch = 36, train mean loss=0.30016764998435974\r",
        "epoch = 37, train mean loss=0.2961408495903015\r",
        "epoch = 38, train mean loss=0.2929992079734802\r",
        "epoch = 39, train mean loss=0.289468377828598\r",
        "epoch = 40, train mean loss=0.2852235436439514\r",
        "epoch = 41, train mean loss=0.28149136900901794\r",
        "epoch = 42, train mean loss=0.2781485915184021\r",
        "epoch = 43, train mean loss=0.27413210272789\r",
        "epoch = 44, train mean loss=0.2700214087963104\r",
        "epoch = 45, train mean loss=0.26648449897766113\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 46, train mean loss=0.2626408040523529\r",
        "epoch = 47, train mean loss=0.2584019899368286\r",
        "epoch = 48, train mean loss=0.25466933846473694\r",
        "epoch = 49, train mean loss=0.25085633993148804\r",
        "epoch = 50, train mean loss=0.246644526720047\r",
        "epoch = 51, train mean loss=0.24276694655418396\r",
        "epoch = 52, train mean loss=0.23892566561698914\r",
        "epoch = 53, train mean loss=0.23472309112548828\r",
        "epoch = 54, train mean loss=0.23085230588912964\r",
        "epoch = 55, train mean loss=0.2269253134727478\r",
        "epoch = 56, train mean loss=0.22279268503189087\r",
        "epoch = 57, train mean loss=0.21888664364814758\r",
        "epoch = 58, train mean loss=0.21668539941310883\r",
        "epoch = 59, train mean loss=0.21304036676883698\r",
        "epoch = 60, train mean loss=0.20900441706180573\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 61, train mean loss=0.2047824263572693\r",
        "epoch = 62, train mean loss=0.19926925003528595\r",
        "epoch = 63, train mean loss=0.19582593441009521\r",
        "epoch = 64, train mean loss=0.19211995601654053\r",
        "epoch = 65, train mean loss=0.18840596079826355\r",
        "epoch = 66, train mean loss=0.18491984903812408\r",
        "epoch = 67, train mean loss=0.18134206533432007\r",
        "epoch = 68, train mean loss=0.17800189554691315\r",
        "epoch = 69, train mean loss=0.1746089607477188\r",
        "epoch = 70, train mean loss=0.17148937284946442\r",
        "epoch = 71, train mean loss=0.16826440393924713\r",
        "epoch = 72, train mean loss=0.16537968814373016\r",
        "epoch = 73, train mean loss=0.16242900490760803\r",
        "epoch = 74, train mean loss=0.1597183644771576\r",
        "epoch = 75, train mean loss=0.15710648894309998\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 76, train mean loss=0.1546025425195694\r",
        "epoch = 77, train mean loss=0.1523100733757019\r",
        "epoch = 78, train mean loss=0.1500745713710785\r",
        "epoch = 79, train mean loss=0.14808061718940735\r",
        "epoch = 80, train mean loss=0.14615392684936523\r",
        "epoch = 81, train mean loss=0.14441384375095367\r",
        "epoch = 82, train mean loss=0.14285297691822052\r",
        "epoch = 83, train mean loss=0.14136120676994324\r",
        "epoch = 84, train mean loss=0.1400715559720993\r",
        "epoch = 85, train mean loss=0.13893702626228333\r",
        "epoch = 86, train mean loss=0.137875497341156\r",
        "epoch = 87, train mean loss=0.1369638890028\r",
        "epoch = 88, train mean loss=0.13621367514133453\r",
        "epoch = 89, train mean loss=0.13555341958999634\r",
        "epoch = 90, train mean loss=0.13496945798397064\r",
        "epoch = 91, train mean loss=0.1344965547323227\r",
        "epoch = 92, train mean loss=0.13413524627685547\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 93, train mean loss=0.13385626673698425\r",
        "epoch = 94, train mean loss=0.13363435864448547\r",
        "epoch = 95, train mean loss=0.13345742225646973\r",
        "epoch = 96, train mean loss=0.13332203030586243\r",
        "epoch = 97, train mean loss=0.133228600025177\r",
        "epoch = 98, train mean loss=0.13317427039146423\r",
        "epoch = 99, train mean loss=0.13314934074878693\r",
        "epoch = 100, train mean loss=0.13314497470855713\r",
        "epoch = 101, train mean loss=0.13315550982952118\r",
        "epoch = 102, train mean loss=0.13317805528640747\r",
        "epoch = 103, train mean loss=0.13321462273597717\r",
        "epoch = 104, train mean loss=0.13327889144420624\r",
        "epoch = 105, train mean loss=0.1334122270345688\r",
        "epoch = 106, train mean loss=0.13369689881801605\r",
        "epoch = 107, train mean loss=0.13415437936782837\r",
        "epoch = 108, train mean loss=0.13442467153072357\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 109, train mean loss=0.13397358357906342\r",
        "epoch = 110, train mean loss=0.1332634836435318\r",
        "epoch = 111, train mean loss=0.13327857851982117\r",
        "epoch = 112, train mean loss=0.1337311863899231\r",
        "epoch = 113, train mean loss=0.13364620506763458\r",
        "epoch = 114, train mean loss=0.1331099271774292\r",
        "epoch = 115, train mean loss=0.1330178678035736\r",
        "epoch = 116, train mean loss=0.13330301642417908\r",
        "epoch = 117, train mean loss=0.13319610059261322\r",
        "epoch = 118, train mean loss=0.13280628621578217\r",
        "epoch = 119, train mean loss=0.13278065621852875\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 120, train mean loss=0.13294008374214172\r",
        "epoch = 121, train mean loss=0.132771298289299\r",
        "epoch = 122, train mean loss=0.132510706782341\r",
        "epoch = 123, train mean loss=0.13254056870937347\r",
        "epoch = 124, train mean loss=0.13259755074977875\r",
        "epoch = 125, train mean loss=0.13241496682167053\r",
        "epoch = 126, train mean loss=0.13226012885570526\r",
        "epoch = 127, train mean loss=0.1323012262582779\r",
        "epoch = 128, train mean loss=0.132284477353096\r",
        "epoch = 129, train mean loss=0.13212355971336365\r",
        "epoch = 130, train mean loss=0.13204573094844818\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 131, train mean loss=0.13207125663757324\r",
        "epoch = 132, train mean loss=0.13201218843460083\r",
        "epoch = 133, train mean loss=0.13188713788986206\r",
        "epoch = 134, train mean loss=0.13184353709220886\r",
        "epoch = 135, train mean loss=0.13184504210948944\r",
        "epoch = 136, train mean loss=0.13177381455898285\r",
        "epoch = 137, train mean loss=0.13167525827884674\r",
        "epoch = 138, train mean loss=0.1316390037536621\r",
        "epoch = 139, train mean loss=0.13161702454090118\r",
        "epoch = 140, train mean loss=0.13154491782188416\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 141, train mean loss=0.1314649134874344\r",
        "epoch = 142, train mean loss=0.13142536580562592\r",
        "epoch = 143, train mean loss=0.13138756155967712\r",
        "epoch = 144, train mean loss=0.1313191056251526\r",
        "epoch = 145, train mean loss=0.13124631345272064\r",
        "epoch = 146, train mean loss=0.1311987191438675\r",
        "epoch = 147, train mean loss=0.13115288317203522\r",
        "epoch = 148, train mean loss=0.1310848444700241\r",
        "epoch = 149, train mean loss=0.1310129165649414\r",
        "epoch = 150, train mean loss=0.13095729053020477\r",
        "epoch = 151, train mean loss=0.13090568780899048\r",
        "epoch = 152, train mean loss=0.13083994388580322\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 153, train mean loss=0.13076558709144592\r",
        "epoch = 154, train mean loss=0.13069958984851837\r",
        "epoch = 155, train mean loss=0.13063938915729523\r",
        "epoch = 156, train mean loss=0.1305743157863617\r",
        "epoch = 157, train mean loss=0.1305021047592163\r",
        "epoch = 158, train mean loss=0.13043062388896942\r",
        "epoch = 159, train mean loss=0.13036219775676727\r",
        "epoch = 160, train mean loss=0.13029423356056213\r",
        "epoch = 161, train mean loss=0.13022300601005554\r",
        "epoch = 162, train mean loss=0.13014915585517883\r",
        "epoch = 163, train mean loss=0.13007450103759766\r",
        "epoch = 164, train mean loss=0.13000071048736572\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 165, train mean loss=0.12992680072784424\r",
        "epoch = 166, train mean loss=0.12985113263130188\r",
        "epoch = 167, train mean loss=0.12977342307567596\r",
        "epoch = 168, train mean loss=0.1296945959329605\r",
        "epoch = 169, train mean loss=0.12961609661579132\r",
        "epoch = 170, train mean loss=0.12953652441501617\r",
        "epoch = 171, train mean loss=0.1294552981853485\r",
        "epoch = 172, train mean loss=0.12937310338020325\r",
        "epoch = 173, train mean loss=0.12929017841815948\r",
        "epoch = 174, train mean loss=0.12920619547367096\r",
        "epoch = 175, train mean loss=0.1291208565235138\r",
        "epoch = 176, train mean loss=0.129034623503685\r",
        "epoch = 177, train mean loss=0.12894709408283234\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 178, train mean loss=0.12885844707489014\r",
        "epoch = 179, train mean loss=0.12876862287521362\r",
        "epoch = 180, train mean loss=0.12867748737335205\r",
        "epoch = 181, train mean loss=0.12858650088310242\r",
        "epoch = 182, train mean loss=0.1284935474395752\r",
        "epoch = 183, train mean loss=0.12839852273464203\r",
        "epoch = 184, train mean loss=0.12830179929733276\r",
        "epoch = 185, train mean loss=0.1282050460577011\r",
        "epoch = 186, train mean loss=0.12810690701007843\r",
        "epoch = 187, train mean loss=0.1280074119567871\r",
        "epoch = 188, train mean loss=0.12790657579898834\r",
        "epoch = 189, train mean loss=0.12780477106571198\r",
        "epoch = 190, train mean loss=0.12770147621631622\r",
        "epoch = 191, train mean loss=0.1275968998670578\r",
        "epoch = 192, train mean loss=0.127491295337677\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 193, train mean loss=0.12738415598869324\r",
        "epoch = 194, train mean loss=0.12727563083171844\r",
        "epoch = 195, train mean loss=0.1271655559539795\r",
        "epoch = 196, train mean loss=0.12705488502979279\r",
        "epoch = 197, train mean loss=0.12694233655929565\r",
        "epoch = 198, train mean loss=0.12682825326919556\r",
        "epoch = 199, train mean loss=0.1267131119966507\r",
        "epoch = 200, train mean loss=0.12659645080566406\r",
        "epoch = 201, train mean loss=0.12647835910320282\r",
        "epoch = 202, train mean loss=0.12635880708694458\r",
        "epoch = 203, train mean loss=0.12623776495456696\r",
        "epoch = 204, train mean loss=0.12611615657806396\r",
        "epoch = 205, train mean loss=0.12599198520183563\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 206, train mean loss=0.12586627900600433\r",
        "epoch = 207, train mean loss=0.12573982775211334\r",
        "epoch = 208, train mean loss=0.12561176717281342\r",
        "epoch = 209, train mean loss=0.125481978058815\r",
        "epoch = 210, train mean loss=0.12535060942173004\r",
        "epoch = 211, train mean loss=0.12521760165691376\r",
        "epoch = 212, train mean loss=0.12508319318294525\r",
        "epoch = 213, train mean loss=0.12494759261608124\r",
        "epoch = 214, train mean loss=0.12481117248535156\r",
        "epoch = 215, train mean loss=0.12467322498559952\r",
        "epoch = 216, train mean loss=0.12453456968069077\r",
        "epoch = 217, train mean loss=0.12439573556184769\r",
        "epoch = 218, train mean loss=0.12425609678030014\r",
        "epoch = 219, train mean loss=0.12411458045244217\r",
        "epoch = 220, train mean loss=0.1239754855632782\r",
        "epoch = 221, train mean loss=0.12383988499641418\r",
        "epoch = 222, train mean loss=0.12370456010103226\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 223, train mean loss=0.12357243150472641\r",
        "epoch = 224, train mean loss=0.12344734370708466\r",
        "epoch = 225, train mean loss=0.12333013117313385\r",
        "epoch = 226, train mean loss=0.12323926389217377\r",
        "epoch = 227, train mean loss=0.12316624820232391\r",
        "epoch = 228, train mean loss=0.1231275424361229\r",
        "epoch = 229, train mean loss=0.12310120463371277\r",
        "epoch = 230, train mean loss=0.12305770814418793\r",
        "epoch = 231, train mean loss=0.1229516789317131\r",
        "epoch = 232, train mean loss=0.1227029412984848\r",
        "epoch = 233, train mean loss=0.12236115336418152\r",
        "epoch = 234, train mean loss=0.12194301933050156\r",
        "epoch = 235, train mean loss=0.12159276753664017\r",
        "epoch = 236, train mean loss=0.12139523029327393\r",
        "epoch = 237, train mean loss=0.12130317091941833\r",
        "epoch = 238, train mean loss=0.12123778462409973\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 239, train mean loss=0.121132493019104\r",
        "epoch = 240, train mean loss=0.12094403058290482\r",
        "epoch = 241, train mean loss=0.12067095190286636\r",
        "epoch = 242, train mean loss=0.12037957459688187\r",
        "epoch = 243, train mean loss=0.12014153599739075\r",
        "epoch = 244, train mean loss=0.11997168511152267\r",
        "epoch = 245, train mean loss=0.11983561515808105\r",
        "epoch = 246, train mean loss=0.11969131231307983\r",
        "epoch = 247, train mean loss=0.11949843168258667\r",
        "epoch = 248, train mean loss=0.1192711740732193\r",
        "epoch = 249, train mean loss=0.11902834475040436\r",
        "epoch = 250, train mean loss=0.11880431324243546\r",
        "epoch = 251, train mean loss=0.11861827969551086\r",
        "epoch = 252, train mean loss=0.11844699084758759\r",
        "epoch = 253, train mean loss=0.11826861649751663\r",
        "epoch = 254, train mean loss=0.11805713176727295\r",
        "epoch = 255, train mean loss=0.11783185601234436\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 256, train mean loss=0.11759715527296066\r",
        "epoch = 257, train mean loss=0.11737507581710815\r",
        "epoch = 258, train mean loss=0.1171710267663002\r",
        "epoch = 259, train mean loss=0.11697551608085632\r",
        "epoch = 260, train mean loss=0.11676513403654099\r",
        "epoch = 261, train mean loss=0.11653833091259003\r",
        "epoch = 262, train mean loss=0.11631052941083908\r",
        "epoch = 263, train mean loss=0.11607766896486282\r",
        "epoch = 264, train mean loss=0.11584776639938354\r",
        "epoch = 265, train mean loss=0.11563006788492203\r",
        "epoch = 266, train mean loss=0.11541798710823059\r",
        "epoch = 267, train mean loss=0.11519216746091843\r",
        "epoch = 268, train mean loss=0.11495320498943329\r",
        "epoch = 269, train mean loss=0.11470355093479156\r",
        "epoch = 270, train mean loss=0.11445542424917221\r",
        "epoch = 271, train mean loss=0.11421491205692291\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 272, train mean loss=0.11397789418697357\r",
        "epoch = 273, train mean loss=0.11374122649431229\r",
        "epoch = 274, train mean loss=0.11349217593669891\r",
        "epoch = 275, train mean loss=0.11323872953653336\r",
        "epoch = 276, train mean loss=0.11298638582229614\r",
        "epoch = 277, train mean loss=0.11273010820150375\r",
        "epoch = 278, train mean loss=0.11247093230485916\r",
        "epoch = 279, train mean loss=0.11221049726009369\r",
        "epoch = 280, train mean loss=0.11195096373558044\r",
        "epoch = 281, train mean loss=0.11169221997261047\r",
        "epoch = 282, train mean loss=0.11142925173044205\r",
        "epoch = 283, train mean loss=0.1111573651432991\r",
        "epoch = 284, train mean loss=0.11088095605373383\r",
        "epoch = 285, train mean loss=0.11060627549886703\r",
        "epoch = 286, train mean loss=0.11033312231302261\r",
        "epoch = 287, train mean loss=0.11005683243274689\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 288, train mean loss=0.10977862775325775\r",
        "epoch = 289, train mean loss=0.10949213802814484\r",
        "epoch = 290, train mean loss=0.10920212417840958\r",
        "epoch = 291, train mean loss=0.10890910774469376\r",
        "epoch = 292, train mean loss=0.10861878842115402\r",
        "epoch = 293, train mean loss=0.10832489281892776\r",
        "epoch = 294, train mean loss=0.10803093016147614\r",
        "epoch = 295, train mean loss=0.10774081200361252\r",
        "epoch = 296, train mean loss=0.10743916034698486\r",
        "epoch = 297, train mean loss=0.1071278378367424\r",
        "epoch = 298, train mean loss=0.10681246966123581\r",
        "epoch = 299, train mean loss=0.10649728775024414\r",
        "epoch = 300, train mean loss=0.10617966949939728\r",
        "epoch = 301, train mean loss=0.1058621034026146\r",
        "epoch = 302, train mean loss=0.1055435985326767\r",
        "epoch = 303, train mean loss=0.10522064566612244\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 304, train mean loss=0.10489529371261597\r",
        "epoch = 305, train mean loss=0.1045634001493454\r",
        "epoch = 306, train mean loss=0.10423068702220917\r",
        "epoch = 307, train mean loss=0.10389459133148193\r",
        "epoch = 308, train mean loss=0.10355423390865326\r",
        "epoch = 309, train mean loss=0.1032169759273529\r",
        "epoch = 310, train mean loss=0.10287026315927505\r",
        "epoch = 311, train mean loss=0.10251887142658234\r",
        "epoch = 312, train mean loss=0.10216976702213287\r",
        "epoch = 313, train mean loss=0.10181296616792679\r",
        "epoch = 314, train mean loss=0.10145556181669235\r",
        "epoch = 315, train mean loss=0.10109344869852066\r",
        "epoch = 316, train mean loss=0.10072682797908783\r",
        "epoch = 317, train mean loss=0.10035973787307739\r",
        "epoch = 318, train mean loss=0.09998728334903717\r",
        "epoch = 319, train mean loss=0.09961111098527908\r",
        "epoch = 320, train mean loss=0.09923269599676132\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 321, train mean loss=0.09885146468877792\r",
        "epoch = 322, train mean loss=0.09846511483192444\r",
        "epoch = 323, train mean loss=0.0980747640132904\r",
        "epoch = 324, train mean loss=0.09768171608448029\r",
        "epoch = 325, train mean loss=0.09728462249040604\r",
        "epoch = 326, train mean loss=0.09688440710306168\r",
        "epoch = 327, train mean loss=0.09648250043392181\r",
        "epoch = 328, train mean loss=0.09607964754104614\r",
        "epoch = 329, train mean loss=0.09566900134086609\r",
        "epoch = 330, train mean loss=0.0952494665980339\r",
        "epoch = 331, train mean loss=0.09482836723327637\r",
        "epoch = 332, train mean loss=0.0944017767906189\r",
        "epoch = 333, train mean loss=0.09397366642951965\r",
        "epoch = 334, train mean loss=0.09354226291179657\r",
        "epoch = 335, train mean loss=0.09310662746429443\r",
        "epoch = 336, train mean loss=0.092666856944561\r",
        "epoch = 337, train mean loss=0.09222312271595001\r",
        "epoch = 338, train mean loss=0.09177443385124207\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 339, train mean loss=0.09132077544927597\r",
        "epoch = 340, train mean loss=0.09086284786462784\r",
        "epoch = 341, train mean loss=0.09040358662605286\r",
        "epoch = 342, train mean loss=0.0899353176355362\r",
        "epoch = 343, train mean loss=0.08946647495031357\r",
        "epoch = 344, train mean loss=0.08899015933275223\r",
        "epoch = 345, train mean loss=0.08851312845945358\r",
        "epoch = 346, train mean loss=0.08802870661020279\r",
        "epoch = 347, train mean loss=0.08753765374422073\r",
        "epoch = 348, train mean loss=0.08703982830047607\r",
        "epoch = 349, train mean loss=0.08654103428125381\r",
        "epoch = 350, train mean loss=0.08603452146053314\r",
        "epoch = 351, train mean loss=0.08551905304193497\r",
        "epoch = 352, train mean loss=0.08499966561794281\r",
        "epoch = 353, train mean loss=0.08447501808404922\r",
        "epoch = 354, train mean loss=0.08394064009189606\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 355, train mean loss=0.08339795470237732\r",
        "epoch = 356, train mean loss=0.08284921944141388\r",
        "epoch = 357, train mean loss=0.08229302614927292\r",
        "epoch = 358, train mean loss=0.08172934502363205\r",
        "epoch = 359, train mean loss=0.08115852624177933\r",
        "epoch = 360, train mean loss=0.08058451861143112\r",
        "epoch = 361, train mean loss=0.08000458776950836\r",
        "epoch = 362, train mean loss=0.0794186070561409\r",
        "epoch = 363, train mean loss=0.07882172614336014\r",
        "epoch = 364, train mean loss=0.07822383940219879\r",
        "epoch = 365, train mean loss=0.07759403437376022\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 366, train mean loss=0.07684823125600815\r",
        "epoch = 367, train mean loss=0.07596764713525772\r",
        "epoch = 368, train mean loss=0.07496194541454315\r",
        "epoch = 369, train mean loss=0.07394963502883911\r",
        "epoch = 370, train mean loss=0.07308167964220047\r",
        "epoch = 371, train mean loss=0.07243217527866364\r",
        "epoch = 372, train mean loss=0.07200264185667038\r",
        "epoch = 373, train mean loss=0.07147922366857529\r",
        "epoch = 374, train mean loss=0.07059174031019211\r",
        "epoch = 375, train mean loss=0.06957842409610748\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 376, train mean loss=0.06874805688858032\r",
        "epoch = 377, train mean loss=0.06784932315349579\r",
        "epoch = 378, train mean loss=0.06712833046913147\r",
        "epoch = 379, train mean loss=0.06643297523260117\r",
        "epoch = 380, train mean loss=0.06569530069828033\r",
        "epoch = 381, train mean loss=0.06498710066080093\r",
        "epoch = 382, train mean loss=0.06408941745758057\r",
        "epoch = 383, train mean loss=0.06322336196899414\r",
        "epoch = 384, train mean loss=0.062290009111166\r",
        "epoch = 385, train mean loss=0.061429958790540695\r",
        "epoch = 386, train mean loss=0.06067845597863197\r",
        "epoch = 387, train mean loss=0.05993352830410004\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 388, train mean loss=0.0592225082218647\r",
        "epoch = 389, train mean loss=0.05845649540424347\r",
        "epoch = 390, train mean loss=0.05777379125356674\r",
        "epoch = 391, train mean loss=0.057033684104681015\r",
        "epoch = 392, train mean loss=0.05619298294186592\r",
        "epoch = 393, train mean loss=0.05524641275405884\r",
        "epoch = 394, train mean loss=0.05430044233798981\r",
        "epoch = 395, train mean loss=0.053412772715091705\r",
        "epoch = 396, train mean loss=0.052611272782087326\r",
        "epoch = 397, train mean loss=0.051877833902835846\r",
        "epoch = 398, train mean loss=0.05116227641701698\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 399, train mean loss=0.050472479313611984\r",
        "epoch = 400, train mean loss=0.049742020666599274\r",
        "epoch = 401, train mean loss=0.04900002479553223\r",
        "epoch = 402, train mean loss=0.048214949667453766\r",
        "epoch = 403, train mean loss=0.04736931249499321\r",
        "epoch = 404, train mean loss=0.04648188129067421\r",
        "epoch = 405, train mean loss=0.04560321941971779\r",
        "epoch = 406, train mean loss=0.04476756975054741\r",
        "epoch = 407, train mean loss=0.043978817760944366\r",
        "epoch = 408, train mean loss=0.04323020949959755\r",
        "epoch = 409, train mean loss=0.04250413179397583\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 410, train mean loss=0.041798584163188934\r",
        "epoch = 411, train mean loss=0.04111555591225624\r",
        "epoch = 412, train mean loss=0.04046635702252388\r",
        "epoch = 413, train mean loss=0.03987405076622963\r",
        "epoch = 414, train mean loss=0.0393393449485302\r",
        "epoch = 415, train mean loss=0.03888838365674019\r",
        "epoch = 416, train mean loss=0.038383398205041885\r",
        "epoch = 417, train mean loss=0.03772299364209175\r",
        "epoch = 418, train mean loss=0.03675905615091324\r",
        "epoch = 419, train mean loss=0.03562562167644501\r",
        "epoch = 420, train mean loss=0.03460521996021271\r",
        "epoch = 421, train mean loss=0.03388252481818199\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 422, train mean loss=0.03340683504939079\r",
        "epoch = 423, train mean loss=0.03300108760595322\r",
        "epoch = 424, train mean loss=0.03251803293824196\r",
        "epoch = 425, train mean loss=0.031850073486566544\r",
        "epoch = 426, train mean loss=0.03102031722664833\r",
        "epoch = 427, train mean loss=0.030148087069392204\r",
        "epoch = 428, train mean loss=0.029374893754720688\r",
        "epoch = 429, train mean loss=0.028760487213730812\r",
        "epoch = 430, train mean loss=0.02825835347175598\r",
        "epoch = 431, train mean loss=0.02780868113040924\r",
        "epoch = 432, train mean loss=0.027347704395651817\r",
        "epoch = 433, train mean loss=0.026843557134270668\r",
        "epoch = 434, train mean loss=0.02625460922718048\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 435, train mean loss=0.025595372542738914\r",
        "epoch = 436, train mean loss=0.02490219660103321\r",
        "epoch = 437, train mean loss=0.024223824962973595\r",
        "epoch = 438, train mean loss=0.023589463904500008\r",
        "epoch = 439, train mean loss=0.023012099787592888\r",
        "epoch = 440, train mean loss=0.022476941347122192\r",
        "epoch = 441, train mean loss=0.02197869122028351\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 442, train mean loss=0.021520216017961502\r",
        "epoch = 443, train mean loss=0.02110360749065876\r",
        "epoch = 444, train mean loss=0.02074633352458477\r",
        "epoch = 445, train mean loss=0.02047382853925228\r",
        "epoch = 446, train mean loss=0.020331619307398796\r",
        "epoch = 447, train mean loss=0.020329756662249565\r",
        "epoch = 448, train mean loss=0.020486457273364067\r",
        "epoch = 449, train mean loss=0.020468242466449738\r",
        "epoch = 450, train mean loss=0.01998250186443329\r",
        "epoch = 451, train mean loss=0.01874525286257267\r",
        "epoch = 452, train mean loss=0.01735895313322544\r",
        "epoch = 453, train mean loss=0.016516344621777534\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 454, train mean loss=0.016442129388451576\r",
        "epoch = 455, train mean loss=0.016694912686944008\r",
        "epoch = 456, train mean loss=0.016637880355119705\r",
        "epoch = 457, train mean loss=0.016019489616155624\r",
        "epoch = 458, train mean loss=0.015070585533976555\r",
        "epoch = 459, train mean loss=0.014360509812831879\r",
        "epoch = 460, train mean loss=0.0141297010704875\r",
        "epoch = 461, train mean loss=0.014164854772388935\r",
        "epoch = 462, train mean loss=0.014104433357715607\r",
        "epoch = 463, train mean loss=0.01371399499475956\r",
        "epoch = 464, train mean loss=0.013098577037453651\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 465, train mean loss=0.012516346760094166\r",
        "epoch = 466, train mean loss=0.012166527099907398\r",
        "epoch = 467, train mean loss=0.012030644342303276\r",
        "epoch = 468, train mean loss=0.01195475272834301\r",
        "epoch = 469, train mean loss=0.01179553847759962\r",
        "epoch = 470, train mean loss=0.011483540758490562\r",
        "epoch = 471, train mean loss=0.011073464527726173\r",
        "epoch = 472, train mean loss=0.010656032711267471\r",
        "epoch = 473, train mean loss=0.010317387990653515\r",
        "epoch = 474, train mean loss=0.01008023414760828\r",
        "epoch = 475, train mean loss=0.009919440373778343\r",
        "epoch = 476, train mean loss=0.009789638221263885\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 477, train mean loss=0.009650632739067078\r",
        "epoch = 478, train mean loss=0.0094859404489398\r",
        "epoch = 479, train mean loss=0.009281594306230545\r",
        "epoch = 480, train mean loss=0.0090500358492136\r",
        "epoch = 481, train mean loss=0.008796129375696182\r",
        "epoch = 482, train mean loss=0.008539948612451553\r",
        "epoch = 483, train mean loss=0.008289002813398838\r",
        "epoch = 484, train mean loss=0.008053990080952644\r",
        "epoch = 485, train mean loss=0.007835575379431248\r",
        "epoch = 486, train mean loss=0.007634414825588465\r",
        "epoch = 487, train mean loss=0.0074468860402703285\r",
        "epoch = 488, train mean loss=0.007270474452525377\r",
        "epoch = 489, train mean loss=0.007102624513208866\r",
        "epoch = 490, train mean loss=0.006941286846995354\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 491, train mean loss=0.006785837933421135\r",
        "epoch = 492, train mean loss=0.006636619567871094\r",
        "epoch = 493, train mean loss=0.006494459696114063\r",
        "epoch = 494, train mean loss=0.006362187210470438\r",
        "epoch = 495, train mean loss=0.00624737236648798\r",
        "epoch = 496, train mean loss=0.006164732854813337\r",
        "epoch = 497, train mean loss=0.006149913650006056\r",
        "epoch = 498, train mean loss=0.006279126740992069\r",
        "epoch = 499, train mean loss=0.006731647532433271\r",
        "epoch = 500, train mean loss=0.00777208898216486\r",
        "epoch = 501, train mean loss=0.00982342753559351\r",
        "epoch = 502, train mean loss=0.0119964350014925\r",
        "epoch = 503, train mean loss=0.01241877768188715\r",
        "epoch = 504, train mean loss=0.00871374923735857\r",
        "epoch = 505, train mean loss=0.005227084271609783\r",
        "epoch = 506, train mean loss=0.006002009846270084\r",
        "epoch = 507, train mean loss=0.008460935205221176\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 508, train mean loss=0.007952325977385044\r",
        "epoch = 509, train mean loss=0.005160449538379908\r",
        "epoch = 510, train mean loss=0.005018367897719145\r",
        "epoch = 511, train mean loss=0.006797609850764275\r",
        "epoch = 512, train mean loss=0.00635489821434021\r",
        "epoch = 513, train mean loss=0.004552585072815418\r",
        "epoch = 514, train mean loss=0.004667885135859251\r",
        "epoch = 515, train mean loss=0.00577008631080389\r",
        "epoch = 516, train mean loss=0.00523443752899766\r",
        "epoch = 517, train mean loss=0.004077224060893059\r",
        "epoch = 518, train mean loss=0.004419849254190922\r",
        "epoch = 519, train mean loss=0.0050371503457427025\r",
        "epoch = 520, train mean loss=0.004373651929199696\r",
        "epoch = 521, train mean loss=0.0037458748556673527\r",
        "epoch = 522, train mean loss=0.004146720748394728\r",
        "epoch = 523, train mean loss=0.004349082242697477\r",
        "epoch = 524, train mean loss=0.0037788015324622393\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 525, train mean loss=0.0035284203477203846\r",
        "epoch = 526, train mean loss=0.003856558818370104\r",
        "epoch = 527, train mean loss=0.0037966768722981215\r",
        "epoch = 528, train mean loss=0.003366702701896429\r",
        "epoch = 529, train mean loss=0.003376902313902974\r",
        "epoch = 530, train mean loss=0.003568480024114251\r",
        "epoch = 531, train mean loss=0.003354374086484313\r",
        "epoch = 532, train mean loss=0.0031145408283919096\r",
        "epoch = 533, train mean loss=0.003213237039744854\r",
        "epoch = 534, train mean loss=0.0032499239314347506\r",
        "epoch = 535, train mean loss=0.0030352275352925062\r",
        "epoch = 536, train mean loss=0.002925354987382889\r",
        "epoch = 537, train mean loss=0.0030073290690779686\r",
        "epoch = 538, train mean loss=0.002976446645334363\r",
        "epoch = 539, train mean loss=0.0028079780749976635\r",
        "epoch = 540, train mean loss=0.002753647742792964\r",
        "epoch = 541, train mean loss=0.0027994548436254263\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 542, train mean loss=0.0027468549087643623\r",
        "epoch = 543, train mean loss=0.002623284235596657\r",
        "epoch = 544, train mean loss=0.0025834282860159874\r",
        "epoch = 545, train mean loss=0.002601706190034747\r",
        "epoch = 546, train mean loss=0.002554765436798334\r",
        "epoch = 547, train mean loss=0.002462803153321147\r",
        "epoch = 548, train mean loss=0.0024254866875708103\r",
        "epoch = 549, train mean loss=0.002428635722026229\r",
        "epoch = 550, train mean loss=0.002391309477388859\r",
        "epoch = 551, train mean loss=0.0023204293102025986\r",
        "epoch = 552, train mean loss=0.00227900268509984\r",
        "epoch = 553, train mean loss=0.0022700177505612373\r",
        "epoch = 554, train mean loss=0.002244219183921814\r",
        "epoch = 555, train mean loss=0.0021904960740357637\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 556, train mean loss=0.0021465858444571495\r",
        "epoch = 557, train mean loss=0.0021283391397446394\r",
        "epoch = 558, train mean loss=0.002109797205775976\r",
        "epoch = 559, train mean loss=0.0020716791041195393\r",
        "epoch = 560, train mean loss=0.002029005903750658\r",
        "epoch = 561, train mean loss=0.0020018231589347124\r",
        "epoch = 562, train mean loss=0.0019842777401208878\r",
        "epoch = 563, train mean loss=0.0019583827815949917\r",
        "epoch = 564, train mean loss=0.0019229109166190028\r",
        "epoch = 565, train mean loss=0.0018912869272753596\r",
        "epoch = 566, train mean loss=0.001869749859906733\r",
        "epoch = 567, train mean loss=0.001850054133683443\r",
        "epoch = 568, train mean loss=0.0018238563789054751\r",
        "epoch = 569, train mean loss=0.001793948351405561\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 570, train mean loss=0.0017681645695120096\r",
        "epoch = 571, train mean loss=0.001748024602420628\r",
        "epoch = 572, train mean loss=0.0017283360939472914\r",
        "epoch = 573, train mean loss=0.0017049830639734864\r",
        "epoch = 574, train mean loss=0.001679726643487811\r",
        "epoch = 575, train mean loss=0.0016567179700359702\r",
        "epoch = 576, train mean loss=0.001637095119804144\r",
        "epoch = 577, train mean loss=0.0016184240812435746\r",
        "epoch = 578, train mean loss=0.001598188653588295\r",
        "epoch = 579, train mean loss=0.0015765837160870433\r",
        "epoch = 580, train mean loss=0.0015556912403553724\r",
        "epoch = 581, train mean loss=0.0015368293970823288\r",
        "epoch = 582, train mean loss=0.0015193626750260592\r",
        "epoch = 583, train mean loss=0.0015016827965155244\r",
        "epoch = 584, train mean loss=0.0014831752050668001\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 585, train mean loss=0.0014644579496234655\r",
        "epoch = 586, train mean loss=0.001446543843485415\r",
        "epoch = 587, train mean loss=0.0014298215974122286\r",
        "epoch = 588, train mean loss=0.0014138304395601153\r",
        "epoch = 589, train mean loss=0.0013978860806673765\r",
        "epoch = 590, train mean loss=0.0013816840946674347\r",
        "epoch = 591, train mean loss=0.0013654467184096575\r",
        "epoch = 592, train mean loss=0.0013496215688064694\r",
        "epoch = 593, train mean loss=0.0013344254111871123\r",
        "epoch = 594, train mean loss=0.0013198552187532187\r",
        "epoch = 595, train mean loss=0.0013056084280833602\r",
        "epoch = 596, train mean loss=0.0012914149556308985\r",
        "epoch = 597, train mean loss=0.001277219969779253\r",
        "epoch = 598, train mean loss=0.0012631448917090893\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 599, train mean loss=0.0012493799440562725\r",
        "epoch = 600, train mean loss=0.0012360477121546865\r",
        "epoch = 601, train mean loss=0.0012231412110850215\r",
        "epoch = 602, train mean loss=0.0012105611385777593\r",
        "epoch = 603, train mean loss=0.0011981707066297531\r",
        "epoch = 604, train mean loss=0.0011858129873871803\r",
        "epoch = 605, train mean loss=0.00117358413990587\r",
        "epoch = 606, train mean loss=0.0011615458643063903\r",
        "epoch = 607, train mean loss=0.0011497600935399532\r",
        "epoch = 608, train mean loss=0.0011382725788280368\r",
        "epoch = 609, train mean loss=0.001127047697082162\r",
        "epoch = 610, train mean loss=0.0011160715948790312\r",
        "epoch = 611, train mean loss=0.0011053005000576377\r",
        "epoch = 612, train mean loss=0.0010947245173156261\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 613, train mean loss=0.0010843361960723996\r",
        "epoch = 614, train mean loss=0.0010741169098764658\r",
        "epoch = 615, train mean loss=0.0010640774853527546\r",
        "epoch = 616, train mean loss=0.0010541294468566775\r",
        "epoch = 617, train mean loss=0.0010441243648529053\r",
        "epoch = 618, train mean loss=0.0010343313915655017\r",
        "epoch = 619, train mean loss=0.001024915254674852\r",
        "epoch = 620, train mean loss=0.0010155767668038607\r",
        "epoch = 621, train mean loss=0.0010066719260066748\r",
        "epoch = 622, train mean loss=0.0009976375149562955\r",
        "epoch = 623, train mean loss=0.000989033724181354\r",
        "epoch = 624, train mean loss=0.0009802071144804358\r",
        "epoch = 625, train mean loss=0.0009719387744553387\r",
        "epoch = 626, train mean loss=0.0009635480237193406\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 627, train mean loss=0.0009556729928590357\r",
        "epoch = 628, train mean loss=0.0009476852137595415\r",
        "epoch = 629, train mean loss=0.0009402176947332919\r",
        "epoch = 630, train mean loss=0.0009327724692411721\r",
        "epoch = 631, train mean loss=0.0009258995996788144\r",
        "epoch = 632, train mean loss=0.0009193401201628149\r",
        "epoch = 633, train mean loss=0.0009135697036981583\r",
        "epoch = 634, train mean loss=0.0009082643082365394\r",
        "epoch = 635, train mean loss=0.0009041884914040565\r",
        "epoch = 636, train mean loss=0.0009017554693855345\r",
        "epoch = 637, train mean loss=0.0009020924917422235\r",
        "epoch = 638, train mean loss=0.0009074518457055092\r",
        "epoch = 639, train mean loss=0.0009206177783198655\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 640, train mean loss=0.0009481388842687011\r",
        "epoch = 641, train mean loss=0.0010008286917582154\r",
        "epoch = 642, train mean loss=0.0010972650488838553\r",
        "epoch = 643, train mean loss=0.001276022638194263\r",
        "epoch = 644, train mean loss=0.001567422179505229\r",
        "epoch = 645, train mean loss=0.0020747582893818617\r",
        "epoch = 646, train mean loss=0.00278229801915586\r",
        "epoch = 647, train mean loss=0.0037794653326272964\r",
        "epoch = 648, train mean loss=0.004418317694216967\r",
        "epoch = 649, train mean loss=0.0044956086203455925\r",
        "epoch = 650, train mean loss=0.0031435026321560144\r",
        "epoch = 651, train mean loss=0.00154475390445441\r",
        "epoch = 652, train mean loss=0.0008104618173092604\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 653, train mean loss=0.0013357127318158746\r",
        "epoch = 654, train mean loss=0.002290662843734026\r",
        "epoch = 655, train mean loss=0.0024785122368484735\r",
        "epoch = 656, train mean loss=0.0018032644875347614\r",
        "epoch = 657, train mean loss=0.0009507749346084893\r",
        "epoch = 658, train mean loss=0.0008322890498675406\r",
        "epoch = 659, train mean loss=0.001358192297630012\r",
        "epoch = 660, train mean loss=0.0017502960981801152\r",
        "epoch = 661, train mean loss=0.0015620725462213159\r",
        "epoch = 662, train mean loss=0.0010020204354077578\r",
        "epoch = 663, train mean loss=0.0007477650651708245\r",
        "epoch = 664, train mean loss=0.0009765686700120568\r",
        "epoch = 665, train mean loss=0.001276384457014501\r",
        "epoch = 666, train mean loss=0.0012550618266686797\r",
        "epoch = 667, train mean loss=0.0009418903500773013\r",
        "epoch = 668, train mean loss=0.0007316936389543116\r",
        "epoch = 669, train mean loss=0.0008159163990058005\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 670, train mean loss=0.0010128665016964078\r",
        "epoch = 671, train mean loss=0.0010562100214883685\r",
        "epoch = 672, train mean loss=0.0008966660825535655\r",
        "epoch = 673, train mean loss=0.0007318967254832387\r",
        "epoch = 674, train mean loss=0.0007225216249935329\r",
        "epoch = 675, train mean loss=0.000830835138913244\r",
        "epoch = 676, train mean loss=0.0009021075675264001\r",
        "epoch = 677, train mean loss=0.000844050373416394\r",
        "epoch = 678, train mean loss=0.0007348807994276285\r",
        "epoch = 679, train mean loss=0.0006853131344541907\r",
        "epoch = 680, train mean loss=0.00072758604073897\r",
        "epoch = 681, train mean loss=0.0007880396442487836\r",
        "epoch = 682, train mean loss=0.0007868674001656473\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 683, train mean loss=0.0007283900049515069\r",
        "epoch = 684, train mean loss=0.0006740372045896947\r",
        "epoch = 685, train mean loss=0.0006715471390634775\r",
        "epoch = 686, train mean loss=0.0007060333155095577\r",
        "epoch = 687, train mean loss=0.000726899306755513\r",
        "epoch = 688, train mean loss=0.0007110817823559046\r",
        "epoch = 689, train mean loss=0.0006727148429490626\r",
        "epoch = 690, train mean loss=0.0006484476616606116\r",
        "epoch = 691, train mean loss=0.0006522355834022164\r",
        "epoch = 692, train mean loss=0.0006688020075671375\r",
        "epoch = 693, train mean loss=0.0006748883752152324\r",
        "epoch = 694, train mean loss=0.0006619280902668834\r",
        "epoch = 695, train mean loss=0.0006408107001334429\r",
        "epoch = 696, train mean loss=0.0006286482093855739\r",
        "epoch = 697, train mean loss=0.0006303751142695546\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 698, train mean loss=0.0006380064296536148\r",
        "epoch = 699, train mean loss=0.0006401932914741337\r",
        "epoch = 700, train mean loss=0.0006327353767119348\r",
        "epoch = 701, train mean loss=0.0006200848729349673\r",
        "epoch = 702, train mean loss=0.000610326009336859\r",
        "epoch = 703, train mean loss=0.0006072816904634237\r",
        "epoch = 704, train mean loss=0.0006090386305004358\r",
        "epoch = 705, train mean loss=0.0006104724598117173\r",
        "epoch = 706, train mean loss=0.0006077919388189912\r",
        "epoch = 707, train mean loss=0.0006011116201989353\r",
        "epoch = 708, train mean loss=0.000593359989579767\r",
        "epoch = 709, train mean loss=0.0005876737996004522\r",
        "epoch = 710, train mean loss=0.000585088157095015\r",
        "epoch = 711, train mean loss=0.000584414170589298\r",
        "epoch = 712, train mean loss=0.0005835523479618132\r",
        "epoch = 713, train mean loss=0.0005810249131172895\r",
        "epoch = 714, train mean loss=0.0005767486873082817\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 715, train mean loss=0.0005716807791031897\r",
        "epoch = 716, train mean loss=0.0005671510007232428\r",
        "epoch = 717, train mean loss=0.000563945563044399\r",
        "epoch = 718, train mean loss=0.0005619986914098263\r",
        "epoch = 719, train mean loss=0.0005605905898846686\r",
        "epoch = 720, train mean loss=0.0005589499487541616\r",
        "epoch = 721, train mean loss=0.0005566640174947679\r",
        "epoch = 722, train mean loss=0.0005538228433579206\r",
        "epoch = 723, train mean loss=0.000550776079762727\r",
        "epoch = 724, train mean loss=0.0005477243685163558\r",
        "epoch = 725, train mean loss=0.0005454142228700221\r",
        "epoch = 726, train mean loss=0.000543937785550952\r",
        "epoch = 727, train mean loss=0.0005428526201285422\r",
        "epoch = 728, train mean loss=0.0005416201893240213\r",
        "epoch = 729, train mean loss=0.0005399939836934209\r",
        "epoch = 730, train mean loss=0.0005378529313020408\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 731, train mean loss=0.0005353993037715554\r",
        "epoch = 732, train mean loss=0.000532910693436861\r",
        "epoch = 733, train mean loss=0.0005305630038492382\r",
        "epoch = 734, train mean loss=0.0005285082152113318\r",
        "epoch = 735, train mean loss=0.0005267804954200983\r",
        "epoch = 736, train mean loss=0.0005252978298813105\r",
        "epoch = 737, train mean loss=0.0005239478778094053\r",
        "epoch = 738, train mean loss=0.0005225907661952078\r",
        "epoch = 739, train mean loss=0.0005211622919887304\r",
        "epoch = 740, train mean loss=0.0005196239217184484\r",
        "epoch = 741, train mean loss=0.0005179948057048023\r",
        "epoch = 742, train mean loss=0.0005163205787539482\r",
        "epoch = 743, train mean loss=0.000514663232024759\r",
        "epoch = 744, train mean loss=0.000513013219460845\r",
        "epoch = 745, train mean loss=0.0005113788065500557\r",
        "epoch = 746, train mean loss=0.0005097806570120156\r",
        "epoch = 747, train mean loss=0.0005082430434413254\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 748, train mean loss=0.0005067692254669964\r",
        "epoch = 749, train mean loss=0.0005053397617302835\r",
        "epoch = 750, train mean loss=0.0005039686802774668\r",
        "epoch = 751, train mean loss=0.0005026280996389687\r",
        "epoch = 752, train mean loss=0.0005013140034861863\r",
        "epoch = 753, train mean loss=0.0005000208620913327\r",
        "epoch = 754, train mean loss=0.0004987529828213155\r",
        "epoch = 755, train mean loss=0.0004972910974174738\r",
        "epoch = 756, train mean loss=0.000495640211738646\r",
        "epoch = 757, train mean loss=0.0004943922976963222\r",
        "epoch = 758, train mean loss=0.0004929644637741148\r",
        "epoch = 759, train mean loss=0.0004920627688989043\r",
        "epoch = 760, train mean loss=0.0004906776011921465\r",
        "epoch = 761, train mean loss=0.0004897018079645932\r",
        "epoch = 762, train mean loss=0.00048819649964571\r",
        "epoch = 763, train mean loss=0.00048721733037382364\r",
        "epoch = 764, train mean loss=0.00048599642468616366\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 765, train mean loss=0.00048528762999922037\r",
        "epoch = 766, train mean loss=0.0004844804934691638\r",
        "epoch = 767, train mean loss=0.00048412050819024444\r",
        "epoch = 768, train mean loss=0.0004839808971155435\r",
        "epoch = 769, train mean loss=0.00048468049499206245\r",
        "epoch = 770, train mean loss=0.0004866525705438107\r",
        "epoch = 771, train mean loss=0.0004907077527604997\r",
        "epoch = 772, train mean loss=0.0004985264386050403\r",
        "epoch = 773, train mean loss=0.0005095566739328206\r",
        "epoch = 774, train mean loss=0.0005296412855386734\r",
        "epoch = 775, train mean loss=0.0005613639950752258\r",
        "epoch = 776, train mean loss=0.0006170500419102609\r",
        "epoch = 777, train mean loss=0.0007079587085172534\r",
        "epoch = 778, train mean loss=0.0008662482723593712\r",
        "epoch = 779, train mean loss=0.0011207847855985165\r",
        "epoch = 780, train mean loss=0.0015575990546494722\r",
        "epoch = 781, train mean loss=0.002156294882297516\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 782, train mean loss=0.003059545299038291\r",
        "epoch = 783, train mean loss=0.003813360817730427\r",
        "epoch = 784, train mean loss=0.0044140322133898735\r",
        "epoch = 785, train mean loss=0.003802367951720953\r",
        "epoch = 786, train mean loss=0.0024841022677719593\r",
        "epoch = 787, train mean loss=0.0009743651025928557\r",
        "epoch = 788, train mean loss=0.000495116866659373\r",
        "epoch = 789, train mean loss=0.0011214454425498843\r",
        "epoch = 790, train mean loss=0.001980963395908475\r",
        "epoch = 791, train mean loss=0.0022340614814311266\r",
        "epoch = 792, train mean loss=0.0015253087040036917\r",
        "epoch = 793, train mean loss=0.000708539504557848\r",
        "epoch = 794, train mean loss=0.0004824850766453892\r",
        "epoch = 795, train mean loss=0.0008868216536939144\r",
        "epoch = 796, train mean loss=0.0013716040411964059\r",
        "epoch = 797, train mean loss=0.0013554535107687116\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch = 798, train mean loss=0.0009303438710048795\r",
        "epoch = 799, train mean loss=0.0005178172141313553\r",
        "[ 0.00223192  0.02236214  0.04233738  0.06183343  0.08238839  0.10195985\n",
        "  0.12274485  0.14234415  0.15925911  0.18119502  0.20062685  0.21978974\n",
        "  0.2396009   0.25816804  0.27725166  0.29686847  0.31629801  0.33553413\n",
        "  0.35448617  0.37092647  0.39005506  0.40917021  0.42752951  0.44578099\n",
        "  0.46262598  0.48038661  0.4981783   0.51579636  0.53278184  0.54976684\n",
        "  0.56675267  0.58317626  0.59948325  0.61486769  0.62994593  0.64502501\n",
        "  0.66006035  0.67458659  0.68911332  0.70409673  0.71949124  0.73465407\n",
        "  0.74642366  0.75889713  0.77169824  0.78449965  0.79730058  0.81010187\n",
        "  0.82205987  0.83302462  0.84385771  0.85452241  0.86402178  0.87352121\n",
        "  0.88302064  0.89252007  0.90201914  0.91151869  0.92101842  0.93051744\n",
        "  0.93711025  0.94298637  0.94886267  0.95473915  0.96031386  0.96570396\n",
        "  0.97109354  0.9764834   0.98116714  0.98417479  0.98718309  0.99019086\n",
        "  0.99319917  0.996207    0.9992153   1.00222349  1.00241768  1.00147843\n",
        "  1.00083292  1.00018728  0.999542    0.99889672  0.99825138  0.99760604\n",
        "  0.99696034  0.99510932  0.99325812  0.99063039  0.98793805  0.98387778\n",
        "  0.97913682  0.97332615  0.96683306  0.96034026  0.95384777  0.94735509\n",
        "  0.94054341  0.93349868  0.92645431  0.91940945  0.91222197  0.90488458\n",
        "  0.89644533  0.88589787  0.87525916  0.86462033  0.85398179  0.84334266\n",
        "  0.83270389  0.82175291  0.81062287  0.79949248  0.78836232  0.77723199\n",
        "  0.76610214  0.75101054  0.73477149  0.71853262  0.70243901  0.68649691\n",
        "  0.6705541   0.65461206  0.63866955  0.62272739  0.606785    0.59084278\n",
        "  0.57490063  0.55895787  0.54301566  0.52707332  0.50949913  0.49115768\n",
        "  0.47281611  0.45447528  0.43613398  0.41779336  0.399452    0.38111031\n",
        "  0.36276913  0.34442812  0.32608694  0.30774614  0.28940481  0.2710638\n",
        "  0.2527222   0.23438078  0.21603954  0.19769871  0.17935732  0.16101632\n",
        "  0.14267531  0.12433358  0.10599258  0.08765184  0.06931042  0.05096918\n",
        "  0.03262787  0.01428651 -0.00405435 -0.02239553 -0.04073716 -0.05907822\n",
        " -0.07741917 -0.0957602  -0.11410133 -0.13244295 -0.15078422 -0.16912502\n",
        " -0.18746647 -0.20580769 -0.22414863 -0.24249029 -0.26083097 -0.27917272\n",
        " -0.29737401 -0.31538755 -0.33340141 -0.35141554 -0.3694292  -0.38744372\n",
        " -0.40545708 -0.42347142 -0.44148561 -0.45949906 -0.47751284 -0.49552631\n",
        " -0.51354039 -0.53155434 -0.54956824 -0.56758243 -0.58559602 -0.60360998\n",
        " -0.62162411 -0.63963795 -0.65765214 -0.67566544 -0.69367939 -0.71169323\n",
        " -0.72970736 -0.74772131 -0.76573527 -0.78374922 -0.801763   -0.81977713\n",
        " -0.83779037 -0.85580474 -0.87381876 -0.89183265 -0.90984577 -0.92786008\n",
        " -0.94587374 -0.94978935 -0.95022428 -0.95065844 -0.95109349 -0.95152789\n",
        " -0.95196241 -0.95239669 -0.95283133 -0.9532662  -0.95370042 -0.95413488\n",
        " -0.95456946 -0.9550041  -0.95543849 -0.95587295 -0.95630771 -0.95674205\n",
        " -0.95717657 -0.9576112  -0.95804608 -0.95848048 -0.95891488 -0.95934892\n",
        " -0.95978367 -0.96021819 -0.96065295 -0.9610877  -0.96152198 -0.96195638\n",
        " -0.96239066 -0.96282506 -0.96326017 -0.96369457 -0.96412921 -0.96456337\n",
        " -0.96499825 -0.96543241 -0.96586752 -0.96630168 -0.96673632 -0.96717095\n",
        " -0.96760476 -0.95852137 -0.94575876 -0.93299431 -0.92023027 -0.9074676\n",
        " -0.89470387 -0.88193989 -0.8691771  -0.85641319 -0.84364969 -0.8308863\n",
        " -0.81812161 -0.80535936 -0.79259473 -0.77983129 -0.76706773 -0.75430393\n",
        " -0.73982072 -0.72455275 -0.70928788 -0.69402039 -0.6787529  -0.66348618\n",
        " -0.64821911 -0.632954   -0.61768544 -0.6024192  -0.58715194 -0.5718857\n",
        " -0.55661917 -0.54135334 -0.52608544 -0.51081938 -0.49555206 -0.48028493\n",
        " -0.46501786 -0.44975251 -0.43448395 -0.419218   -0.40395021 -0.38868314\n",
        " -0.37341738 -0.35815004 -0.34288377 -0.32761714 -0.31235045 -0.29708284\n",
        " -0.28181601 -0.26654947 -0.25128365 -0.23601544 -0.22075075 -0.20548251\n",
        " -0.19021714 -0.17494896 -0.15968239 -0.14441484 -0.12914947 -0.11388261\n",
        " -0.0986153  -0.08334836]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "done.\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD7CAYAAABpJS8eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4FFUXx/HvTaGFEgFBJEBARBCQokKwYAClKIKFKkgv\n0jsB6b1JVTqhK4igIFIEJQFFKRaqBEIJ0ovUJEhCct8/EngBk7CbbHJ3sufzPHnY2Z2d+XEJJ5Oz\nM3eU1hohhBDpn5vpAEIIIdKGFHwhhHARUvCFEMJFSMEXQggXIQVfCCFchBR8IYRwER6mAzxMKSXn\niQohhJ201upR6zjlEb7W2pJfQ4YMMZ5B8pvPIfmt+WXl/LZyyoJvVWFhYaYjpIjkN0vym2X1/LaQ\ngi+EEC5CCr4DtWjRwnSEFJH8Zkl+s6ye3xbKnv5PWlBKaWfLJIQQzkwphbbqh7ZWFRwcbDpCikh+\nsyS/WVbPbwsp+EII4SKkpSOEEBYnLR0hhBAPkILvQFbvAUp+syS/WVbPbwsp+EII4SKkhy+EEBYn\nPXwhhBAPcEjBV0oFKqUuKKX2JbHONKVUqFJqj1KqrCP262ys3gOU/GZJfrOsnt8WjjrCXwDUSOxF\npVQt4Cmt9dNAe2CWg/YrhBDCRg7r4SulCgFrtdbPJfDaLCBIa/1l/PIhwF9rfSGBdaWHL4QQdrC1\nh59WN0DJD5y6b/lM/HP/KfjivyKjI9lyPIifjv1O1L8ZiL2VFc8Yb7Jn9CZvdm+K+sR95fLyxsvT\nC6Ue+e8uhHBBTnfHK4ibtc7X1xcAb29vypYti7+/P/D/PpszLt/fA0zp9gqWLsxnm9bx5YbFnNf7\ncMv8Ih5nX8Yz8hCemW+RsXBWot2vEX46jCjCiS0cjcp8FY+TGSiYuTjv1qlJzRKvok9oPNw90jy/\n1cdf8kt+03mSWr772N45/E21dEKA19JbSyc4OPjeP0xynA8/z4A1n/HNodVci76I9+Va+D/5Fg2e\nr07lCt7kyweJHbzfvg379mnW/XSGjft3s+fiLjKW+BF373N0rdSBXq90IVvGbKma3zTJb5bkN8fW\nlo4jC74vcQW/dAKvvQl00lq/pZTyA6Zorf0S2Y5lC35yXbh5iZYLRrHpwmIyhzblw7JN6NXoRZ4q\nkvzP1CMiYN06+GzlHnZ5TiDj09uYVGMSrfzqSctHiHQmTQu+UuoLwB/IRVxffgiQAdBa6znx63wG\n1AQigJZa6z8S2ZbLFPzI6Eg6L53C4tBJ5DrXmDG1BtDsvSfwcHCj7fBh6P3pNtarjhTOlZ9v283i\n2ScLO3YnQghjbC34xm++m8DNeLVVBQUF2bTenZg7etymQJ2xf36dtWV9PW1pqI6NTd1sWmsdejxK\nP9dxrHbrl0t/OH2qjr5z54HXbc3vrCS/WZLfnPi6+cj6KlfaprGfT/5CodHPM2DlAlp5reLK7BV0\naVI00d68IxUt7Mne6QEsrbKdVYdWkKdfZX46FJL6OxZCOAWZSyeNxOpYOq/uR+Duz/E9PJHVIxtS\nooS5Xvq/t2OpO3IGm6OH0qLIIOa17YKbkp//QlhRmn9o6yjpseDfib1D7Tlt2LL3CD3yfsfoQTlx\ndzedKs7KLUdpuupD8ub0YmuPhfjm9DEdSQhhJ5k8zYD7z5G9KzLqFmXHvE/Q7vOsrLOZcUOdp9gD\n1KtalL+H/USGs1Uo2rUUc39ZYTpSsiU0/lYi+c2yen5bSMFPRf+EX+fpYbU4fSIzBwd+S51aXqYj\nJShPbg9C5gzgDTWOjl8H0GXVMNORhBCpQFo6qeTExYuU/aQm2W/6sW/Mpzzm7USH9UmYMvcivQ9U\noU3FD5j1wQDTcYQQNnC2uXRcyv5TJ6nw6Rs8E92IXVOHkSGDdS506t42D7m++YEW2yoT828W5rbq\nYTqSEMJBpKXjQMHBwew8dpgXpr/KC7oTf0wcbqlif7eH+eG7+VhV50fmH5xKpwWzzYayg9V7sJLf\nLKvnt4UUfAf668wpXp1bjSpqKNvGd8PNwqP7TpWCfFX7R2b9NYKei5aYjiOEcADp4TvInydDqTiz\nKtXchrF+VKs0uZAqLXy55RAfbKxK3+emMaZpfdNxhBAJkB5+Ggq9cIpKs6rx0u1hrJ+Ufoo9QMOq\nJYiO2UizH6qTc01m+tStbTqSECKZLNx0cA7XIyN4YVIdStzowqDaRSxd7BPrYTZ9owxTKqwl4JdW\nzP3xh7QNZQer92Alv1lWz28LKfgpEBMbS7nhzcgaXoadk3s71QVVjtb1/QoMeHoVH21uzKrd203H\nEUIkg/TwU+CN0YP45VwQYcN+5PGcGU3HSRNtx37PwuvN2NZ2M5WK/OdeN0IIA2RqhVTWI3AZQVeW\n8HOXr12m2APMCajBq+HTqBpYiyOXjpmOI4SwgxT8ZPg8eBdTQ7uy9K1vKVcsz73nrd4DtCW/UrBx\nYkMKnBhMhc+qc+7mudQPZiNXGH9nJvmdnxR8Ox08dYbm69+je5F5NKrimi2NDBng10/b4763NRU/\nq8HN2zdNRxJC2EB6+HYIvx1J/oGVKeVWj+3j+pmOY9zevZqKY1ryzlteLP9wuuk4Qrgs6eE7mNaa\nSmPakOlmCYJGBJiO4xTKlFFMrjGZVQfWsP6vrabjCCEeQQq+jXosWkjI1f3sGDgn0flxrN4DTE7+\nDi0fo9q/M2iwtDURUZGOD2UHVxx/ZyL5nZ8UfBtsOxDKtJC+LHhrGYV9MpuO43RWj6tDhssv8PbE\noaajCCGSID38R7gVFUWe/i9T5bHmfDuws+k4Tuu3QxepsLA0C15fT/M3njcdRwiXIj18B6kxZggZ\no/Pydb9OpqM4tRdK5KHT0xNo910brl6PNh1HCJEAKfhJ+HRtENsjFvFj1/l4eDx6khyr9wBTmn9a\n6w/JnTkP1YdOdEwgO7n6+Jsm+Z2fFPxEHD/3Dz1+asag0gsoUzTPo98gUErxfefZ/JHpEz77ItR0\nHCHEQ6SHnwCtNb59GuLtnp+94yYbzWJFPb6cwvQfV3N00BYKFpBjCiFSm/TwU6DngmWcjzlI0IAx\npqNY0if1u5An/y1q9J9HbKzpNEKIu6TgP2TfiTNMO9KdGdUXkzN7Jrvea/UeoKPyu7u5s7btPI4W\nHMD4mWcdsk1byPibJfmdnxT8+2itqTGjNRXdOtG6lpxamBLlnixNm7IdGLyzE2FhztU2FMJVSQ//\nPm1nz2LJwXlcHP0r2bN6GsmQnty+c5sCo8qRL2QEe75439J3AxPCmUkP3047jxwj8MQgFtZdIsXe\nQTJ6ZGRl83kcKtSVaXOvmo4jhMuTgg/ExMZQO7A5/m4f06haiWRvx+o9wNTIX9n3Jd4v+S4BP/Th\n1CmHb/4BMv5mSX7nJwUf6LDwU8JvuPPtgG6mo6RLcxqMIVPJzdTr+yNO1kEUwqW4fA8/5OwpSn5a\njrmVttOqzjNptl9Xs+av9TRY0JX5L+ynSUOZgE4IR7K1h+/yBf/pge+R5eZz7J06NM326apen9WI\nnRuLcHrRaHLkMJ1GiPRDPrS1weT1azkRcYDv+jnm7lVW7wGmdv6lTadw57l5tB+8L1W2L+NvluR3\nfi5b8K9HRtBvaxd6PTOLAvnsu8BKJM8TWZ9gzBuj+Dq6Hb/siDEdRwiX45CWjlKqJjCFuB8ggVrr\ncQ+9/hqwBjge/9TXWuuRiWwrTVo61cd9zL6TJzk3/XM5PzwNxepYnh3vT/jOBoSt6IyHh+lEQlhf\nmrV0lFJuwGdADaAk0FgpVTyBVbdprcvHfyVY7NPKL4eP8MPVOSxvPUGKfRpzU2583XI2F58dyrAp\nqXyephDiAY5o6VQAQrXWJ7XW0cByoG4C6zlFadVa02B+d15VAfg//6RDt231HmBa5X82Twk+KteF\ncfs7c/as436bk/E3S/I7P0cU/PzA/Ydqp+Ofe1glpdQepdQ6pdSzDthvsnzy7XdciDrO1wFyzr1J\nE+r2I5vvEZqMWG06ihAuI8U9fKXU+0ANrXW7+OWmQAWtddf71skKxGqtI5VStYCpWutiiWwv1Xr4\nEbf/JeegZ+nx9CzGtq2eKvsQtlt78Efend+OH979C/9XMpqOI4Rl2drDd8RHZmeAgvct+8Q/d4/W\nOvy+xxuUUjOUUjm11lcS2mCLFi3w9fUFwNvbm7Jly+Lv7w/8/9eu5Cw3+nQCmY74UKNWhnv7Ssn2\nZDlly2+XrEbhiDzUH9id8z/OxN3dufLJsiw76/Ldx2FhYdhFa52iL8AdOAoUAjIAe4ASD62T977H\nFYCwJLanU8Ofx09q1S+n3vDriVTZvtZaBwUFpdq204KJ/H9dPKQ9Ps6tJ868lOJtyfibJfnNia+b\nj6zXKe7ha61jgM7AJuAgsFxrfUgp1V4p1S5+tXpKqQNKqT+JO32zYUr3a6/3Z/XlRd2Zmn6+ab1r\nkYQSjxfn/WcaMnDzMK4k+PueEMJRXGJqhcAffqLdxiac6neIJ3N7OXTbIuUuRVyi4PgSvHftZz6f\nmtAZvUKIpMjUCvHuxMTQfWM3WvqMl2LvpB73epx+r/Rj5fW+7EudWReEELhAwe+2cCGxt7Mws1Pq\nd5Hu/0DFikzm71elC9meOkCL4cmfQlnG3yzJ7/zSdcG/eP06s48MZHzVqXh6OsV1XyIRGT0y8lnd\n8fzl04u162SeHSFSQ7ru4fuP7sPxs1f5+7N5DtmeSF1aa0pNepUrP7bi7zWt8JQ7TQphE5fv4f92\n7ATbbs7n89ZGp+0RdlBKMb/hJK6UHcinc8If/QYhhF3SbcH/YN5AXoztyqvlnkizfVq9B+gM+Sv6\nVOD1p6oyaMN4rl2z773OkD8lJL9ZVs9vi3RZ8Fdu/4NjMUGs6N7LdBSRDDPrjSam/HT6jZHZNIVw\npHTZw8/d8w38877HyoAODkol0lr3bwcy84u/CRmzmMKFTacRwrm5bA9//KpNXOcki7q2MR1FpMCI\nGgFkKLGZdkN/Mx1FiHQjXRX8mNhYhm0PoFPx0XhlTvtTPKzeA3Sm/NkyZmNs9eFsy9yLX3+17Tc+\nZ8qfHJLfLKvnt0W6Kvg95i+DmIxMbP2+6SjCAT6q2Io8Ba/S5pPVyb4YSwjxf+mmhx9+6zaPDS7O\nOL9F9Hy/ciokEyZsPLKZOnM68JX/X9StneHRbxDCBblcD7/F9Bl4R5WSYp/O1Cz2Bs/lf4YO86cT\nIxfgCpEi6aLgn758jW8uj2Hme2ON5rB6D9BZ8y9qMoFLJUYze/E/Sa7nrPltJfnNsnp+W6SLgt94\n+jh8o96m3mslTUcRqaBknmd5u0gDAjYM599/TacRwros38Pfc/w05eeU4eeme3mplE8qJhMmXYq4\nhM/YEvTIsZ2xvZ8xHUcIp+IyPfwP5g6hvGorxT6de9zrcbqUD2Dygb52T7kghIhj6YK/4feDhMSu\nZXmnfqajANbvATp7/pFvdyFDgf10mhCU4OvOnv9RJL9ZVs9vC0sX/DbL+lHDqx9FfbxNRxFpIJNH\nJsa9PpYvr/fk71Nyyo4Q9rJsD3/O99vouLkZFwcdJmeOjGmQTDgDrTU+Q16m6PV2bJ3awnQcIZxC\nuu7ha63puzmApvlHSrF3MUopFjSaxE8ZBrD3UITpOEJYiiUL/shVX3Mr+hazOn5gOsoDrN4DtEr+\n6s/6UTrbazSdMeGB562SPzGS3yyr57eF5Qp+1J1oRu/qT9cS48iU0XLxhYMsazOGg1k/ZfPOM6aj\nCGEZluvhd108h8BfV3D90814eMiNyV3ZG2M/5vDZs/w9baHpKEIYlS57+LfvRDH74Gg+rjRcir3g\n8w79OJP5exZt+t10FCEswVIFv/vCRWQMf4b+TV8yHSVBVu8BWi1/nhzZaZxvGN039kJrbbn8D5P8\nZlk9vy0sU/BvRUUReHg0gysPwc0yqUVqm9uhFZGx/zBy5RrTUYRwepbp4X/42TS+O7yOK9O+R0k3\nR9yn39xNTA3tzLXRB8joIXPmC9eTrnr4p69e4IszI5hcc4oUe/Efo1pVx+NGUbosmmk6ihBOzRIF\nv9m8UeT/pykt3iphOkqSrN4DtGp+d3cY5T+BwC1DuBxxxXScZLPq+N8l+Z2f0xf80Iun2Hrlc2Y3\n7W86inBiXRqWJNuVyjSfP8J0FCGcltP38F8a055LJ3MSOmuMwVTCClasu0jjn59lf/dfeDZvMdNx\nhEgz6aKHf+DMcXZcX0Vg6z6mowgLqP9mHgqd6UPTRQGmowjhlJy64DefP5xnwztT+cWcpqPYxOo9\nQKvn37o1mHlturHv4h42HQk2HcduVh9/ye/8nLbg7zoewp8R61jYvofpKMJCqlbOROkLY2mzohex\nOtZ0HCGcitP28EsOaUzm68/x2xT5sFbY548/NH7zXmJ6iw60rdDMdBwhUp2le/hbQ/Zz6FYQizp1\nMR1FWFD58opXwifTZ8MAIqJkznwh7nLKgt9m6RBe0n0p+XRW01HsYvUeYHrKP72/H/8eeYVRQRPN\nBbJTehp/K7J6fls4pOArpWoqpUKUUkeUUgmeIqGUmqaUClVK7VFKlU1qe8du72Rx1w6OiCZcVIkS\n8FamMUz5dRpnb541HUcIp5DiHr5Syg04AlQDzgK7gUZa65D71qkFdNZav6WUqghM1Vr7JbI9Xa3/\nZ/wwulOKcglx/DiU7N6P95pe5PMG803HESLVpGUPvwIQqrU+qbWOBpYDdR9apy6wGEBrvRPIoZTK\nm9gGF3dv44BYwtUVKQKNffqz+uB6/jz3p+k4QhjniIKfHzh13/Lp+OeSWudMAuvc82Qea96Y3Oo9\nwPSYf8SAHBA8lC7fxc2Z78zS4/hbidXz28LDdICEtGjRAl9fXwC8vb0pW7Ys/v7+wP//UWRZlm1Z\nDg0Nplbep9ny9wXWHllL9nPZnSqfLMtycpbvPg4LC8Mejujh+wFDtdY145f7AVprPe6+dWYBQVrr\nL+OXQ4DXtNYXEthekve0FcJely5BkRobefzDbhzuegBPd0/TkYTFXbgA2bJBliymk8RJyx7+bqCo\nUqqQUioD0Aj49qF1vgWaxQfzA64lVOyFSA2PPw7d3qxJ1IXCzPptluk4Ih3o1g1mWvD2Cyku+Frr\nGKAzsAk4CCzXWh9SSrVXSrWLX2c9cEIpdRSYDXRM6X6d0f2/bllRes7fuzdEfPMJw4JGcvXW1bQL\nZYf0PP5WYGv+ffsgOBg++ihV46QKh/TwtdYbgWceem72Q8udHbEvIZLD2xv6NC/F/PPvMnLbSCbW\nsM4FWcK5DB4M/fqBl5fpJPZz2rl0hHC08HAoXPoCd9qVZHf7HRTNWdR0JGExu3fDu+/C0aOQKZPp\nNP9n6bl0hEgNWbPCgG55yXeiNwE/yJz5wn6DBsHAgc5V7O0hBd+BXKWH6axsyf/RR3BjU3d+OfE7\n205uS/1QdnCF8Xdmj8r/009w+DC0agXrjqxj87HNaRPMgaTgC5eSKRMM6p+J3HvG0PP7njJnvrCJ\n1nFH9kOGwPXoS7RZ2wavDNZr4ksPX7ic6Gh4prgmU6dK9K/WiQ/LfGg6knBymzdD585w8CB88E0D\nfL19Gf/GeNOx7rG1hy8FX7ikJUtgwvJfuPp6Qw53PkwWTye5gkY4Ha2hYkXo2RNUqS8ZtnUYf7T/\ng0weztPIlw9tDUjvPUxnZ0/+Dz6AmLCXKOzxEpN+nZR6oezgSuPvjBLLv2YNREVB5TfP03VjVxa9\ns8ipir09pOALl+TuDsOHw5UVY5myYwrnbp4zHUk4oZiYuN79yJGajus/ok25NryY/0XTsZJNWjrC\nZWkNL7wABVoFkLvAP8yrM890JOFkli6Nm0LhoxlLmfDLeHa33U1GD+ebzVd6+ELYYMMG6N7vOtc/\nfIbvm35PmSfKmI4knERUVNyd08bNPEPHveXY2HQj5fOVNx0rQdLDNyC99jCtIjn5a9aE3FlzUDPz\nEHpu6ml0znxXHH9n8nD++fPhqaKaBZfb0fHFjk5b7O0hBV+4NKVg1CjYNrktZ2+cY13oOtORhBO4\ndQtGjAC/jxZw9uZZBrw6wHQkh5CWjhDAG29Aibc3sMmtB/s77Jc5813cJ5/Aj7/9zW/PP8+WZlso\nnbe06UhJkpaOEHYYNQq+nlATn2wFmf377Ee/QaRbN27AuPGaG/5t6OHXw+mLvT2k4DtQeuthWk1K\n8leoAM+XV5S/PJER20Zw7d9rjgtmI1cef2dwN/+kSVC0wRyi3a/R9+W+ZkM5mBR8IeKNGAGLJ5Tm\nzSJ1GbVtlOk4woDLl2HKohMc8RnIwncW4uHmlLf9Tjbp4Qtxn0aNoHDp88z1LMXONjt5KudTpiOJ\nNNSrdyzLM1aj+5tv0uflPqbj2EzOwxciGQ4fhldegY8+H03I9T/5qv5XpiOJNHL6NDzz4aeUaLiM\nne1+wt3N3XQkm8mHtgaklx6mVTki/zPPwNtvQ9S2Huw6s4uf//455cFsJONvVrPOn6NfG8YX9Rda\nqtjbQwq+EA8ZMgTmzcxM3+dlznxXEXI4hq0xYxlYeRDFchUzHSfVSEtHiAT07g3hEbH8+Xwlulbo\nSpPnmpiOJFJRmY8mcePJNRwbFISbst5xsPTwhUiBf/6B4sVh2jfbCdjdmJDOITJnfjr15Y8hfPDD\nK+zrsouSTxYxHSdZpIdvgNV7mJL//3LlijvKXzn5Zfx8/Jj862SHbTsxMv5pLzrmDm3XNafxE8O5\ndORv03FSnRR8IRLRtSvs2gUNc41l8o7JnA8/bzqScLA2Cz4h5t+szO/4kekoaUJaOkIkYf58WLgQ\nKgzsw43b15nz9hzTkYSD7D13gOc/rcLs8r/Rul4h03FSRFo6QjhA8+Zw5Qo8HzGANYfXsO/CPtOR\nhANEx0RTd0FzCh8fTav3rV3s7SEF34Gs2MO8n+T/L3d3GDMGRg70ZuCrg+m1qVeqzZkv4592RgSN\n5fzxx1ncrQ0q/rjYSvmTSwq+EI9Qu3bch7gZD7Tj9I3TbDi6wXQkkQJ7zu9h0vZPqRo+j0qVHtkF\nSVekhy+EDXbsgPr1Yer6dQzY1pt9H+2TOfMtKComivIzX+Tklz35PbA5xdLJNVbSwxfCgfz84qZQ\nDln7Jj7ZfZj7x1zTkUQyjNg6gsjzBWlaulm6Kfb2kILvQFbvAUr+pI0bB5MmKfqXn8iwrcMcPme+\njH/q+u3sb8zcPYdri+cwdMh/D4adPb8jSMEXwkZFi8adtbN86nPUKVaH0T+NNh1J2OjfO//SfHVz\nCh+eTJ8O+cib13QiM6SHL4Qdrl2Lm1Fz2drzNAguxa62uyjymDUvx3cl/X7oxy+HQzk5YSWHQxSZ\nMplO5FjSwxciFXh7w+DBMPrjJ+ju14OAHwJMRxKPsOP0DhbuWciVxTOZMD79FXt7SMF3IKv3ACW/\nbdq3h7NnofjVnuw+s5utYVsdsl0Zf8e7FX2L5qub826mT/H2zEP9+omv64z5HU0KvhB28vCAiRNh\nQN/MjK4ynh7f9yAmNsZ0LJGAgVsGUip3OdaMrs/kydy7yMpVSQ9fiGTQGmrWhFq1NCuzv0qrcq1o\nVa6V6VjiPj+d/ImGKxvS6Op+Lp3MxZIlphOlnjSZD18p9RjwJVAICAMaaK2vJ7BeGHAdiAWitdYV\nktimFHxhCQcOQNWq8EXwbzTbUIfDnQ+TLWM207EEEBEVQZlZZQgoN5F+79Rl717w8TGdKvWk1Ye2\n/YAftNbPAFuA/omsFwv4a63LJVXsrc7qPUDJb59SpeKuvv36sxeo/lR1xvw8JkXbk/F3nH4/9KNS\ngUr88FldOne2rdg7U/7UktKCXxdYFP94EfBOIuspB+xLCKczYgSsWgWN84xmzu9zOHH1hOlILi/o\nRBDfhHzD+1mmsXMnBMiJVPektKVzRWudM7Hl+54/DlwDYoA5WutEr0uXlo6wmnnz4ubNrzVmJPsu\n7uWr+l+ZjuSybt6+yXOznmPKG9PpX+9NRo2Cd981nSr1Oaylo5TarJTad9/X/vg/6ySwemKV+mWt\ndXngTaCTUuqVR+1XCKto1Qru3IEnjvdi15ldbDu5zXQkl9Vncx+q+lYldP2bFCwI7yTWc3BRHo9a\nQWv9RmKvKaUuKKXyaq0vKKWeAC4mso1z8X9eUkp9A1QAfk5suy1atMDX1xcAb29vypYti7+/P/D/\nPpszLt/fA3SGPJI/7fY/fbo/depk5sPBzWk9tTUhn4Tg7uZumfxWH39/f382HdvE1xu+5pNX59Nz\nLPzyC2zdap389izffRwWFoZdtNbJ/gLGAQHxjwOAsQmskwXIGv/YC9gOVE9im9qqgoKCTEdIEcmf\nMm3bat21W6x+KfAlPf+P+Xa/33T+lDKZ/9qta7rApAJ609FNunFjrfv3t38bVh7/+Lr5yJqd0h5+\nTmAFUAA4SdxpmdeUUvmAuVrr2kqpwsA3xLV7PIDPtdZjk9imTkkmIUy5fBmefRamfrWbXr/VldM0\n01CrNa3I4J6Bxtlm8eGHcOgQeHmZTpV20uQ8/NQgBV9Y2cyZsHw5+HZvTv7s+RldTWbUTG3rjqyj\n84bO/N56H69VysaQIVCvnulUaUsmTzPg/v6aFUn+lGvXDiIioOw/o5n9+2y7TtN0hvwpYSL/lVtX\naP9de+bXmc+C2dnIlw/efz9527L6+NtCCr4QDuTuDnPnwtgB+WlburvMppnKum3sxnsl3sOXKowZ\nAzNmyHw5SZGWjhCpoE8fOH3hFr+UL87Sd5fyaqFXTUdKd1aHrKbP5j782W4P9ep64e8P/fqZTmWG\n9PCFMCgiIm7qhcajlvN9+AR2t92Nm5JfqB3lcuRlSs8szVf1v+Lvn19h3Dj47TfwdNH7yksP3wCr\n9wAlv+N4ecW1F5YNbIinysjivYsf+R5nyp8caZm/0/pOfFDqA0p4vUKvXnFttJQWe6uPvy2k4AuR\nSmrVgkp+iqePTWHAlgHcvH3TdKR0YcXBFew9v5eRVUfSpw80aAAV0u2UjI4lLR0hUtGFC1CmDJQf\n2YxyhQswqtoo05Es7UL4BcrMKsPqRqu5eciPNm3ipqnO5uKXO0hLRwgnkDcvTJ0KoTNHM+u3WYRd\nCzMdybIPKb8MAAAUHklEQVS01nRY14EWZVtQIpsfrVvHTVzn6sXeHlLwHcjqPUDJnzoaNIAyhX0o\ncaNbkqdpOmt+W6V2/i/2f8GRf44wzH8YPXvCm2/CG4nO9GU/q4+/LR45eZoQImWUivsAt3T53hzN\nXpyf//6ZVwrKhLH2OHvzLD2+78GGJhv44fuMBAXB3r2mU1mP9PCFSCNffQVd5y4j33sT+a39LjlN\n00Zaa95e9jbl85Wne5nhPPccLF0K8RNICqSHL4TTqV8fKudsxLkznjadpiniLNq7iNM3TjOw8kC6\ndYP33pNin1xS8B3I6j1AyZ/6ZkxXxKybQu/1AwiPCn/gNSvkT0pq5D91/RR9Nvdh0TuLWLUiA7t2\nwZiU3To4UVYff1tYpofv6+vLyZMnTcdwCYUKFbL/xgrCJrlywZcTK/LWvKoM+n4sk98eaTqS09Ja\n02ZtG7pW6Eq2yDJ06wbff+9a0x47mmV6+PE9KgOJXI+MderrMuA0s1UZDvf8g8I5C5mO45Tm/D6H\nOb/PYVuzX6nq70mjRtC9u+lUzindzaUjRSjtyFinvuhoKNxyOI+X+Is/Byw3HcfphF0L44U5L7C1\nxVY+n1KSvXvhu+9kJszEyIe2wuVYqQfr6QnfD+7Nvqu/ELg57vbOVsqfEEflj9WxtFrTij4v9eHC\ngZIsWgQLFqR+sbf6+NtCCr4QhpQsloWOxcbS8dvuXP4n1nQcpzF913QioyNp7NuLDz+EhQshTx7T\nqdIHaemI/5CxTjtaa54c/BKPn/yIPQub4+bih2A7Tu+gzrI6bGn6M+3rFaNWLRg40HQq5yctHSfW\noUMHRo2ybRIte9ZNysmTJ3FzcyM2Vo4knYlSiq9aT+awz8cMGBb+6DekY+dunqPeinoE1glkxohi\nPP44fPyx6VTpixzhu4iTJ09SpEgRoqOjcXvEYaRVxzo4OBh/i16RU++LpqyfoVjRbwm1a5tOkzwp\nGf+omCiqLKpC9SLVKXhiCGPHwu7dkD27YzMmxcrfP3KE76TkCFskZErtsajia2je7SRHjphOk/a6\nbejG41kep5bXIAICYPXqtC32rkIKvoOEhIRQpUoVHnvsMUqXLs3atWsBaNmyJR07duStt94iW7Zs\nBAcH07JlSwYPHnzvvePHj+fJJ5/Ex8eHwMBA3NzcOH78+L33311369atFChQgEmTJpE3b17y58/P\nwoUL721n/fr1lC9fnhw5clCoUCGGDRuWdgPgBKx6dAbgk92Hvk178VT7AGrXhn/+MZ3Ifskd/3l/\nzCMoLIgxFRbz/ntuzJ4NJUo4NpstrPz9Yysp+A5w584d3n77bWrWrMmlS5eYNm0aTZs2JTQ0FIBl\ny5YxaNAgbt68ycsvv/zAezdu3MiUKVPYsmULR48eJTg4GJXE+Wfnz5/n5s2bnD17lnnz5tGpUyeu\nX78OQNasWVmyZAnXr19n3bp1zJo1i2+//Tb1/uLCofq83IdzntupUG87774Lt2+bTpT6dpzeQf8f\n+7O09moavZudrl3h3XdNp0q/0k3BV8oxX8mxY8cOIiIiCAgIwMPDgypVqlC7dm2++OILAOrWrYuf\nnx8AGTNmfOC9X331FS1btqR48eJkypSJoUOHJrmvDBkyMGjQINzd3alVqxZZs2bl8OHDAFSuXJmS\nJUsCUKpUKRo1asTWrVuT95eyIKufR71r+y7GVhvLYd/u5MkbS+vWYKWPUuwd/7sf0s55K5BBHYpT\nqRL07p062Wxh9e8fW6Sbgq+1Y76S4+zZsxQoUOCB5woWLMiZM2cA/vNaUu8tUKBAkh+Y5sqV64EP\nXbNkyUJ4eNzZHTt37qRq1arkyZMHb29vZs+ezeXLl5P1dxJmNC7dGHc3d2r2XcqxY9C3r7WKvq2i\nYqKo/1V92pRry3cT6wDw2WdyJW1qSzcF36Qnn3ySU6dOPfDc33//jY+PD0CSLZp8+fJx+vTpB96X\n1PpJadKkCe+88w5nzpzh2rVrtG/f3pJn2ySX1Xuw/v7+uCk3ptScwpCf+vPlN+Fs3Jh6s0M6mj3j\n321DN3JlzsW1bwdx6FDcvQI8DE/laPXvH1tIwXeAihUrkiVLFsaPH8+dO3cIDg7mu+++o1GjRo98\nb4MGDViwYAEhISFERkYycmTyZ08MDw/nsccew9PTk127dt1rKd3lSsXfyvx8/PD39WfuX+PYtAnm\nz4fp002ncpy7H9I+e3gJwUFurFsHWbOaTuUapOA7gKenJ2vXrmX9+vXkzp2bzp07s2TJEooVK5bg\n+vcfwdesWZOuXbtSpUoVihUrRqVKlYD/9voTc/+2ZsyYwaBBg8iRIwcjR46kYcOGia6bHlm9B3t/\n/rHVxjLjtxlEZT7J5s0wbhzMnm0umy1sGf8dp3fw8Y8f8+aN1Xy9LDubNsFjj6V+NltY/fvHJlpr\np/qKi/RfiT2f3hw6dEh7eHjomJgYYxmsOtZBQUGmI6TIw/mHBA3RjVY20lprffSo1oUKaT11atrn\nstWjxv/czXPaZ5KPbjBojS5eXOszZ9Iml62s/P0T/3/2kfVVrrR1AqtXr+bNN98kIiKCFi1a4OHh\nwapVq4zlSc9jbSURUREUn16cL+t9yUsFXuLkSahWDdq1i/sw10qiYqKouqgqscdeJ+K7oWzeLBOi\nOZJcaWshs2fPJk+ePDz99NN4enoyY8YM05GEE/DK4MWYamPovrE7sTqWQoVg61ZYtAi6dYOYGNMJ\nbddlXXdOhuQievNggoKk2JsiBd8JbNiwgWvXrnH58mVWrlxJ3rx5TUeyJKv3YBPK/0HpD1BKsXTf\nUgDy54ft2+HAAXj/fYiISOOQSUhs/Kf9HMiS7VsocyzuQ9qcOdM2l62s/v1jCyn4QjgxN+XGlBpT\n+PjHj+/d9NzbGzZsiPuzcmU4ccJwyCQs/3kHPdf1o6FezZoV2eV+tIZJD1/8h4y182nydROeeuwp\nhlcZfu85rWHaNBg9GubOhTp1DAZMwKcLztP94It0KjydaZ2cLFw6I/e0FckmY+18Tl0/RdnZZfmz\n/Z8UzFHwgdd27ICGDeG992DUKMiSxVDIeDduQJfuUazIXJUWlV9nZsOhZgO5APnQVrgcq/dgk8pf\nIEcBOr/YmR7f9+BW9K0HXvPzgz/+gAsXoEyZuA92TYi74BBKlYJdubpTxS8n0xsMfvQbnYTVv39s\nYfhiZiGErfq+3JdGqxrx2LjH8MrghZenF1k8s9x77PWWF49X9KLW3CwUXOvFa5W8eCKnV8LrJvJc\nZo/MybpA7/hxGDYMTp2ChmPns/bKFpbV34mbkmNKZ5Kilo5Sqh4wFCgBvKi1/iOR9WoCU4j7jSJQ\naz0uiW1KS8cwGWvn9u+dfwmPCiciKoKI6AgioyPvPY6IiuDS9QjWrI/kp50RlPeLoNyLEcS4x68X\nv879f97//tt3bpPZM/MDPxQS/OEQ/3zs7SxsD/Zizy4valT1onzlc3z6+0S2tthK8dzFTQ+Vy0iT\nHr5S6hkgFpgN9E6o4Cul3IAjQDXgLLAbaKS1Dklkm5Yr+L6+vly8eBEPDw+yZs1KjRo1mD59OllS\n2Ext2bIlX3zxBRkzZoy7Sk4pAgMDqV+/Pr6+vty6dYuwsDAyZ84MQGBgIEuXLiUoKAgANzc3Spcu\nzd69e+9tc9CgQZw5c4b58+cnul9nHmthu9On4466V66E+vWhSxcoXTrp98TExhAZHZngD4e7z4VH\nRfBXaARB2yM4FBrJs2UjKFU+glj3CGJ0DIMrD6bE4wbuYOLC0qSHr7U+rLUOBZLaUQUgVGt9Umsd\nDSwH6qZkv85GKcW6deu4ceMGe/bs4c8//2SMg6Y4DAgI4MaNG9y8eZMbN25Qv379e/uMjY1lypQp\n/8lyv7Nnz7J8+XKHZHF2Vu/BOjq/j0/c2TshIVCgANSoAeXKxf0Q2LsXErrbprubO9kyZiNv1rwU\neawIpfOWxs/Hjyq+1chz7W32L2vElGatWfNxV5oU7M/Zz0fwx5hJLK4/mzY527Ds/WWWLfZW//6x\nRVo02PID988dfDr+uXTl7hFxnjx5qFGjBnv27AEgKiqK3r17U6hQIfLly0fHjh25fd+tjJK6veGj\n9OnTh4kTJ3Ljxo1E1+nbty+DBw+We+m6sLx5YdCguP761Klw/TrUqxc3adlrr8VdtTtuHAQGwtdf\nx/1GsGQJfPIJdOoEL70EOXJAq1ZxPyTmzoXQ0LiblTjrRVQiYY/80FYptRm4/9JPBWhggNZ6bWqE\natGiBb6+vgB4e3tTtmzZ1NhNqjh9+jQbNmzg9ddfB+KO0E+cOMG+ffvw8PDggw8+YPjw4YwaNeqB\n2xv6+vrStm1buz4we+GFF/D392fChAmMGDHiP68rpXjvvfdYsWIFCxcupFWrVnb/fe4e9dydK9yZ\nl/39/Z0qj7Pld3eH2Nhg6tSBSZP8uXIFFiwI5tgx+Ocff0JCIDQ0GDc3KFDAnyeeAKWCadAA1q/3\nx9s7bnu3b4ObW9rnT+1lK+W/+zgsLAx7OOQ8fKVUENArkR6+HzBUa10zfrkfcTO7JfjBbXJ7+GqY\nY6b+1UPsH4/ChQvzT/xdp8PDw6lWrRqrVq0ie/bsZM2alf3791O4cGEAfv31V5o0acLx48dp3bo1\nTzzxBKNGjQLg2LFjFCtWjNDQUIoUKULLli1Zvnw5mTNnRmuNp6cnFy9evLfPwMBA8ubNyyuvvMLR\no0dZvXo1n3/+OVu2bAHievhHjx7l8OHDdOzYkdDQUIYNGyY9fCHSGVt7+I48LTOxne0GiiqlCgHn\ngEZAYwfuF0heoXakNWvWUKVKFbZt20aTJk24fPkyt2/fJjIykueff/7eerGxsfeK6dmzZ3nxxRfv\nvZbQ7Q379OnD8OHDSUzJkiWpXbs2Y8aMoUSJhHuntWrVwsfHh1mzZqXkr+j0goODLX3XIslvltXz\n2yJFPXyl1DtKqVOAH/CdUmpD/PP5lFLfAWitY4DOwCbgILBca30oZbGdz91CXblyZZo3b07v3r3J\nnTs3WbJk4eDBg1y5coUrV65w7do1rl+/Djju9oZDhw5l7ty59+6hm5CRI0cyevRoIiMj7d6+ECJ9\nSOlZOqu11gW01pm11vm01rXinz+nta5933obtdbPaK2f1lqPTWloZ9e9e3c2b97M/v37adu2Ld27\nd+fSpUsAnDlzhk2bNgGOu73hU089RcOGDZk2bVqi67z22muUKlWKRYsWJWsfVmD1ozPJb5bV89tC\nLoNzgIePynPnzk2zZs0YMWIE48aNo2jRovj5+eHt7U316tU5cuQIkLLbGz68z8GDBxMZGfnA8w+v\nM3LkSK5evZrub3UohEiYTJ7mREJCQihdujS3b9/Gzc3cz2KrjrXVe7CS3ywr55fJ0yxi9erVREVF\ncfXqVQICAqhTp47RYi+ESL/kCN+wWrVq8euvv+Lh4YG/vz/Tp083fser9DrWQqRXMh++SDYZayGs\nRVo6wuVYfS4UyW+W1fPbQgq+EEK4CGnpiP+QsRbCWkxMrZCqChUqJOePp5FChQqZjiCESAWWaemE\nhYWhtXbqr6CgIOMZHJHf3hn4nIXVe7CS3yyr57eFZQq+FdydA9+qJL9Zkt8sq+e3hRR8B7p27Zrp\nCCki+c2S/GZZPb8tpOALIYSLkILvQFbtfd8l+c2S/GZZPb8tnPK0TNMZhBDCaiw5tYIQQojUIS0d\nIYRwEVLwhRDCRThNwVdK1VRKhSiljiilAkznsYdSKlApdUEptc90luRQSvkopbYopQ4qpfYrpbqa\nzmQPpVRGpdROpdSf8fmHmM5kL6WUm1LqD6XUt6az2EspFaaU2hs//rtM57GXUiqHUuorpdSh+P8D\nFU1nspVSqlj8uP8R/+f1pP7/OkUPXynlBhwBqgFngd1AI611iNFgNlJKvQKEA4u11s+ZzmMvpdQT\nwBNa6z1KqazA70Bdq4w/gFIqi9Y6UinlDmwHumqtLVN8lFI9gOeB7FrrOqbz2EMpdRx4Xmt91XSW\n5FBKLQS2aq0XKKU8gCxa6xuGY9ktvo6eBipqrU8ltI6zHOFXAEK11ie11tHAcqCu4Uw201r/DFjy\nmx1Aa31ea70n/nE4cAjIbzaVfbTWkfEPMxI3R5T5IxkbKaV8gDeBeaazJJPCeWqJXZRS2YFXtdYL\nALTWd6xY7OO9DhxLrNiD8/wj5QfuD3kaixWc9EIp5QuUBXaaTWKf+JbIn8B5YLPWerfpTHaYDPTB\nQj+kHqKBzUqp3UqptqbD2KkwcFkptSC+LTJHKZXZdKhkaggsS2oFZyn4wgnEt3NWAt3ij/QtQ2sd\nq7UuB/gAFZVSz5rOZAul1FvAhfjfsFT8l9W8rLUuT9xvKZ3iW5xW4QGUB6bH/x0igX5mI9lPKeUJ\n1AG+Smo9Zyn4Z4CC9y37xD8n0kh873IlsERrvcZ0nuSK/3U8CKhpOouNXgbqxPfBlwFVlFKLDWey\ni9b6XPyfl4BviGvRWsVp4JTW+rf45ZXE/QCwmlrA7/H/BolyloK/GyiqlCqklMoANAKsdraCVY/O\n7poP/KW1nmo6iL2UUrmVUjniH2cG3gAs8YGz1vpjrXVBrXUR4r7vt2itm5nOZSulVJb43wxRSnkB\n1YEDZlPZTmt9ATillCoW/1Q14C+DkZKrMY9o54CT3ABFax2jlOoMbCLuh1Cg1vqQ4Vg2U0p9AfgD\nuZRSfwND7n4IZAVKqZeBJsD++D64Bj7WWm80m8xm+YBF8WcpuAFfaq3XG87kKvIC38RPieIBfK61\n3mQ4k726Ap/Ht0WOAy0N57GLUioLcR/Ytnvkus5wWqYQQojU5ywtHSGEEKlMCr4QQrgIKfhCCOEi\npOALIYSLkIIvhBAuQgq+EEK4CCn4QgjhIqTgCyGEi/gfjQTfMm252bkAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x17fbc50f748>"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "[<matplotlib.lines.Line2D at 0x17fbc4ab710>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x17fbd574358>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}