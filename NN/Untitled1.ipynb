{
 "metadata": {
  "name": "",
  "signature": "sha256:00d490e79165eaf833e49f6777790eb68fa34ccd1424899ba4809e1cbd366e28"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import json\n",
      "import numpy as np\n",
      "import random\n",
      "import pandas as pd\n",
      "import pickle\n",
      "\n",
      "import argparse\n",
      "import math\n",
      "import sys\n",
      "import time\n",
      "import numpy as np\n",
      "from chainer import cuda, Variable, FunctionSet, optimizers\n",
      "import chainer.functions  as F\n",
      "\n",
      "mod = np\n",
      "n_epoch   = 20 # # 39   # number of epochs\n",
      "n_units   = 200 # 650  # number of units per layer\n",
      "batchsize = 20   # minibatch size\n",
      "bprop_len = 35   # length of truncated BPTT\n",
      "grad_clip = 5    # gradient norm threshold to clip\n",
      "\n",
      "INPUT_LEN = 20\n",
      "OUTPUT_LEN = 20\n",
      "TRAINING_DIV = 200\n",
      "\n",
      "# Prepare RNNLM model\n",
      "# model = FunctionSet(embed=F.EmbedID(INPUT_LEN, n_units),\n",
      "#                     l1_x =F.Linear(n_units, 4 * n_units),\n",
      "#                     l1_h =F.Linear(n_units, 4 * n_units),\n",
      "#                     l2_x =F.Linear(n_units, 4 * n_units),\n",
      "#                     l2_h =F.Linear(n_units, 4 * n_units),\n",
      "#                     l3   =F.Linear(n_units, OUTPUT_LEN))\n",
      "model = FunctionSet(embed=F.EmbedID(4, n_units),\n",
      "                    l1_x =F.Linear(n_units, 4 * n_units),\n",
      "                    l1_h =F.Linear(n_units, 4 * n_units),\n",
      "                    l2_x =F.Linear(n_units, 4 * n_units),\n",
      "                    l2_h =F.Linear(n_units, 4 * n_units),\n",
      "                    l3   =F.Linear(n_units, 4))\n",
      "for param in model.parameters:\n",
      "    param[:] = np.random.uniform(-0.1, 0.1, param.shape)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Neural net architecture\n",
      "def forward_one_step(x_data, y_data, state, train=True):\n",
      "    x = Variable(x_data, volatile=not train)\n",
      "    t = Variable(y_data, volatile=not train)\n",
      "    h0     = model.embed(x)\n",
      "    h1_in  = model.l1_x(F.dropout(h0, train=train)) + model.l1_h(state['h1'])\n",
      "    c1, h1 = F.lstm(state['c1'], h1_in)\n",
      "    h2_in  = model.l2_x(F.dropout(h1, train=train)) + model.l2_h(state['h2'])\n",
      "    c2, h2 = F.lstm(state['c2'], h2_in)\n",
      "    y      = model.l3(F.dropout(h2, train=train))\n",
      "    state  = {'c1': c1, 'h1': h1, 'c2': c2, 'h2': h2}\n",
      "    if train == True:\n",
      "        return state, F.softmax_cross_entropy(y, t)\n",
      "    else:\n",
      "        return state, F.softmax(y)\n",
      "\n",
      "def make_initial_state(batchsize=batchsize, train=True):\n",
      "    return {name: Variable(mod.zeros((batchsize, n_units), dtype=np.int32),\n",
      "                           volatile=not train)\n",
      "            for name in ('c1', 'h1', 'c2', 'h2')}\n",
      "\n",
      "# Setup optimizer\n",
      "optimizer = optimizers.SGD(lr=1.)\n",
      "optimizer.setup(model.collect_parameters())\n",
      "\n",
      "# Evaluation routine\n",
      "# def evaluate(dataset):\n",
      "#     sum_log_perp = mod.zeros(())\n",
      "#     state        = make_initial_state(batchsize=1, train=False)\n",
      "#     for i in xrange(dataset.size - 1):\n",
      "#         x_batch = dataset[i  :i+1]\n",
      "#         y_batch = dataset[i+1:i+2]\n",
      "#         state, loss   = forward_one_step(x_batch, y_batch, state, train=False)\n",
      "#         sum_log_perp += loss.data.reshape(())\n",
      "#     return math.exp(cuda.to_cpu(sum_log_perp) / (dataset.size - 1))\n",
      "\n",
      "UP = 3\n",
      "BIT_UP = 2\n",
      "# EVEN = 2\n",
      "BIT_DOWN = 1\n",
      "DOWN = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "c:\\program files\\python35\\lib\\site-packages\\chainer-1.9.0-py3.5-win-amd64.egg\\chainer\\function_set.py:62: FutureWarning: 'collect_parameters' is deprecated. You can pass FunctionSet itself to 'optimizer.setup'\n",
        "  warnings.warn(msg, FutureWarning)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rates_fd = open('daily_sales_cout.csv', 'r')\n",
      "exchange_rates = []\n",
      "prev = 0\n",
      "for line in rates_fd:\n",
      " ''' \n",
      "    splited = line.split(\",\")\n",
      "    if splited[2] != \"High\" and splited[0] != \"<DTYYYYMMDD>\"and splited[0] != \"2007/1/1\" and splited[0] != \"2007/1/1\":\n",
      "        time_str = splited[0].replace(\"/\", \"-\") + \" \" + splited[1]\n",
      "        if prev == 0:\n",
      "            val = BIT_UP\n",
      "            prev = float(splited[2])\n",
      "        else:\n",
      "            diff = float(splited[2]) - prev\n",
      "            prev = float(splited[2])\n",
      "            if diff > 0.15:\n",
      "                val = UP\n",
      "            elif diff < -0.15:\n",
      "                val = DOWN\n",
      "            elif diff >= 0:\n",
      "                val = BIT_UP\n",
      "            elif diff < 0:\n",
      "                val = BIT_DOWN\n",
      "            else:\n",
      "                val = BIT_UP # guard\n",
      "                \n",
      "        exchange_rates.append([time_str, val])\n",
      " '''\n",
      "DATA_LEN = len(exchange_rates)\n",
      "\n",
      "# Learning loop\n",
      "train_len    = DATA_LEN / TRAINING_DIV\n",
      "cur_log_perp = mod.zeros(())\n",
      "epoch        = 0\n",
      "start_at     = time.time()\n",
      "cur_at       = start_at\n",
      "state        = make_initial_state()\n",
      "accum_loss   = Variable(mod.zeros(()))\n",
      "\n",
      "print (\"data size: \" + str(DATA_LEN))\n",
      "print (\"train_len: \" + str(train_len))\n",
      "print (\"input len: \" + str(INPUT_LEN))\n",
      "print (\"output len: \" + str(OUTPUT_LEN))\n",
      "print (\"epoch num: \" + str(n_epoch))\n",
      "\n",
      "print ('going to train {} iterations'.format(train_len * n_epoch))\n",
      "for epoch in range(n_epoch):\n",
      "    for i in range(int(train_len)):\n",
      "        x_batch = np.array([exchange_rates[(INPUT_LEN + OUTPUT_LEN) * i + j][1]\n",
      "                              # for j in xrange(batchsize)], dtype=np.int32)\n",
      "                               for j in range(INPUT_LEN)], dtype=np.int32)\n",
      "                            \n",
      "        y_batch = np.array([exchange_rates[(INPUT_LEN + OUTPUT_LEN) * i + j + 1][1]\n",
      "                              # for j in xrange(batchsize)], dtype=np.int32)\n",
      "                               for j in range(OUTPUT_LEN)], dtype=np.int32)\n",
      "        state, loss_i = forward_one_step(x_batch, y_batch, state)\n",
      "        accum_loss   += loss_i\n",
      "        cur_log_perp += loss_i.data.reshape(())\n",
      "\n",
      "        if (epoch * train_len + i) % bprop_len == 0:  # Run truncated BPTT\n",
      "            optimizer.zero_grads()\n",
      "            accum_loss.backward()\n",
      "            accum_loss.unchain_backward()  # truncate\n",
      "            accum_loss = Variable(mod.zeros(()))\n",
      "\n",
      "            optimizer.clip_grads(grad_clip)\n",
      "            optimizer.update()\n",
      "\n",
      "        if (epoch * train_len + i) % 1000 == 0:\n",
      "            now      = time.time()\n",
      "            throuput = 1000. / (now - cur_at)\n",
      "            perp     = math.exp(cuda.to_cpu(cur_log_perp) / 1000)\n",
      "            print ('iter {} training perplexity: {:.2f} ({:.2f} iters/sec)'.format(i + 1, perp, throuput))\n",
      "            cur_at   = now\n",
      "            cur_log_perp.fill(0)\n",
      "\n",
      "        # if (i * epoch + 1) % jump == 0:\n",
      "        if epoch * train_len + i % 100 == 0:            \n",
      "            now  = time.time()\n",
      "            # perp = evaluate(valid_data)\n",
      "            # print 'epoch {} validation perplexity: {:.2f}'.format(epoch, perp)\n",
      "            print('epoch {}, i {}'.format(epoch, i))\n",
      "            cur_at += time.time() - now  # skip time of evaluation\n",
      "\n",
      "        if epoch >= 6:\n",
      "            optimizer.lr /= 1.2\n",
      "            print('learning rate =', optimizer.lr)\n",
      "\n",
      "        sys.stdout.flush()\n",
      "\n",
      "# Evaluate on test dataset\n",
      "print ('test')\n",
      "portfolio = 1000000\n",
      "LONG = 1\n",
      "SHORT = 2\n",
      "NOT_HAVE = 3\n",
      "pos_kind = NOT_HAVE\n",
      "positions = 0\n",
      "\n",
      "trade_val = -1\n",
      "\n",
      "result = None\n",
      "state = make_initial_state(batchsize=1, train=False)\n",
      "for i in range(int(DATA_LEN - (DATA_LEN / TRAINING_DIV))):\n",
      "    cur_idx = int((DATA_LEN / TRAINING_DIV) + i)\n",
      "    x_batch = np.array([exchange_rates[cur_idx + j][1]for j in range(INPUT_LEN)])\n",
      "    # y_batch = np.array([exchange_rates[i + INPUT_LEN + j][1]\n",
      "    #                        for j in xrange(OUTPUT_LEN)])\n",
      "    state, loss_i = forward_one_step(x_batch, x_batch, state, train=False)\n",
      "\n",
      "    print (\"last of input\\n\")\n",
      "    print (x_batch)\n",
      "    print (\"loss_i\\n\")\n",
      "#print (loss_i.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "data size: 0\n",
        "train_len: 0.0\n",
        "input len: 20\n",
        "output len: 20\n",
        "epoch num: 20\n",
        "going to train 0.0 iterations\n",
        "test\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DATA_LEN"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}