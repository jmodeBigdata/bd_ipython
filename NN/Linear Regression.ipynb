{
 "metadata": {
  "name": "",
  "signature": "sha256:347069a12bc566ebbb084911085e58dc213397592a7db721397061f5ad63d0d1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gzip\n",
      "import os\n",
      "\n",
      "import numpy as np\n",
      "import six\n",
      "from six.moves.urllib import request\n",
      "\n",
      "parent = 'http://yann.lecun.com/exdb/mnist'\n",
      "train_images = 'train-images-idx3-ubyte.gz'\n",
      "train_labels = 'train-labels-idx1-ubyte.gz'\n",
      "test_images = 't10k-images-idx3-ubyte.gz'\n",
      "test_labels = 't10k-labels-idx1-ubyte.gz'\n",
      "num_train = 60000\n",
      "num_test = 10000\n",
      "dim = 784\n",
      "\n",
      "\n",
      "def load_mnist(images, labels, num):\n",
      "    data = np.zeros(num * dim, dtype=np.uint8).reshape((num, dim))\n",
      "    target = np.zeros(num, dtype=np.uint8).reshape((num, ))\n",
      "\n",
      "    with gzip.open(images, 'rb') as f_images,\\\n",
      "            gzip.open(labels, 'rb') as f_labels:\n",
      "        f_images.read(16)\n",
      "        f_labels.read(8)\n",
      "        for i in six.moves.range(num):\n",
      "            target[i] = ord(f_labels.read(1))\n",
      "            for j in six.moves.range(dim):\n",
      "                data[i, j] = ord(f_images.read(1))\n",
      "////\n",
      "    return data, target\n",
      "\n",
      "\n",
      "def download_mnist_data():\n",
      "    print('Downloading {:s}...'.format(train_images))\n",
      "    request.urlretrieve('{:s}/{:s}'.format(parent, train_images), train_images)\n",
      "    print('Done')\n",
      "    print('Downloading {:s}...'.format(train_labels))\n",
      "    request.urlretrieve('{:s}/{:s}'.format(parent, train_labels), train_labels)\n",
      "    print('Done')\n",
      "    print('Downloading {:s}...'.format(test_images))\n",
      "    request.urlretrieve('{:s}/{:s}'.format(parent, test_images), test_images)\n",
      "    print('Done')\n",
      "    print('Downloading {:s}...'.format(test_labels))\n",
      "    request.urlretrieve('{:s}/{:s}'.format(parent, test_labels), test_labels)\n",
      "    print('Done')\n",
      "\n",
      "    print('Converting training data...')\n",
      "    data_train, target_train = load_mnist(train_images, train_labels,\n",
      "                                          num_train)\n",
      "    print('Done')\n",
      "    print('Converting test data...')\n",
      "    data_test, target_test = load_mnist(test_images, test_labels, num_test)\n",
      "    mnist = {'data': np.append(data_train, data_test, axis=0),\n",
      "             'target': np.append(target_train, target_test, axis=0)}\n",
      "\n",
      "    print('Done')\n",
      "    print('Save output...')\n",
      "    with open('mnist.pkl', 'wb') as output:\n",
      "        six.moves.cPickle.dump(mnist, output, -1)\n",
      "    print('Done')\n",
      "    print('Convert completed')\n",
      "\n",
      "\n",
      "def load_mnist_data():\n",
      "    if not os.path.exists('mnist.pkl'):\n",
      "        download_mnist_data()\n",
      "    with open('mnist.pkl', 'rb') as mnist_pickle:\n",
      "        mnist = six.moves.cPickle.load(mnist_pickle)\n",
      "    return mnist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import chainer\n",
      "import chainer.functions as F\n",
      "import chainer.links as L\n",
      "\n",
      "\n",
      "class MnistMLP(chainer.Chain):\n",
      "\n",
      "    \"\"\"An example of multi-layer perceptron for MNIST dataset.\n",
      "\n",
      "    This is a very simple implementation of an MLP. You can modify this code to\n",
      "    build your own neural net.\n",
      "\n",
      "    \"\"\"\n",
      "    def __init__(self, n_in, n_units, n_out):\n",
      "        super(MnistMLP, self).__init__(\n",
      "            l1=L.Linear(n_in, n_units),\n",
      "            l2=L.Linear(n_units, n_units),\n",
      "            l3=L.Linear(n_units, n_out),\n",
      "        )\n",
      "\n",
      "    def __call__(self, x):\n",
      "        h1 = F.relu(self.l1(x))\n",
      "        h2 = F.relu(self.l2(h1))\n",
      "        return self.l3(h2)\n",
      "\n",
      "\n",
      "class MnistMLPParallel(chainer.Chain):\n",
      "\n",
      "    \"\"\"An example of model-parallel MLP.\n",
      "\n",
      "    This chain combines four small MLPs on two different devices.\n",
      "\n",
      "    \"\"\"\n",
      "    def __init__(self, n_in, n_units, n_out):\n",
      "        super(MnistMLPParallel, self).__init__(\n",
      "            first0=MnistMLP(n_in, n_units // 2, n_units).to_gpu(0),\n",
      "            first1=MnistMLP(n_in, n_units // 2, n_units).to_gpu(1),\n",
      "            second0=MnistMLP(n_units, n_units // 2, n_out).to_gpu(0),\n",
      "            second1=MnistMLP(n_units, n_units // 2, n_out).to_gpu(1),\n",
      "        )\n",
      "\n",
      "    def __call__(self, x):\n",
      "        # assume x is on GPU 0\n",
      "        x1 = F.copy(x, 1)\n",
      "\n",
      "        z0 = self.first0(x)\n",
      "        z1 = self.first1(x1)\n",
      "\n",
      "        # sync\n",
      "        h0 = z0 + F.copy(z1, 0)\n",
      "        h1 = z1 + F.copy(z0, 1)\n",
      "\n",
      "        y0 = self.second0(F.relu(h0))\n",
      "        y1 = self.second1(F.relu(h1))\n",
      "\n",
      "        # sync\n",
      "        y = y0 + F.copy(y1, 0)\n",
      "        return y\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "\"\"\"Chainer example: train a multi-layer perceptron on MNIST\n",
      "This is a minimal example to write a feed-forward net.\n",
      "\"\"\"\n",
      "from __future__ import print_function\n",
      "import argparse\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "import six\n",
      "\n",
      "import chainer\n",
      "from chainer import computational_graph\n",
      "from chainer import cuda\n",
      "import chainer.links as L\n",
      "from chainer import optimizers\n",
      "from chainer import serializers\n",
      "\n",
      "import data\n",
      "#import net\n",
      "\n",
      "\n",
      "\n",
      "batchsize = args.batchsize\n",
      "n_epoch = args.epoch\n",
      "n_units = args.unit\n",
      "\n",
      "print('GPU: {}'.format(args.gpu))\n",
      "print('# unit: {}'.format(args.unit))\n",
      "print('# Minibatch-size: {}'.format(args.batchsize))\n",
      "print('# epoch: {}'.format(args.epoch))\n",
      "print('Network type: {}'.format(args.net))\n",
      "print('')\n",
      "\n",
      "# Prepare dataset\n",
      "print('load MNIST dataset')\n",
      "mnist = data.load_mnist_data()\n",
      "mnist['data'] = mnist['data'].astype(np.float32)\n",
      "mnist['data'] /= 255\n",
      "mnist['target'] = mnist['target'].astype(np.int32)\n",
      "\n",
      "N = 60000\n",
      "x_train, x_test = np.split(mnist['data'],   [N])\n",
      "y_train, y_test = np.split(mnist['target'], [N])\n",
      "N_test = y_test.size\n",
      "\n",
      "# Prepare multi-layer perceptron model, defined in net.py\n",
      "if args.net == 'simple':\n",
      "    model = L.Classifier(net.MnistMLP(784, n_units, 10))\n",
      "    if args.gpu >= 0:\n",
      "        cuda.get_device(args.gpu).use()\n",
      "        model.to_gpu()\n",
      "    xp = np if args.gpu < 0 else cuda.cupy\n",
      "elif args.net == 'parallel':\n",
      "    cuda.check_cuda_available()\n",
      "    model = L.Classifier(net.MnistMLPParallel(784, n_units, 10))\n",
      "    xp = cuda.cupy\n",
      "\n",
      "# Setup optimizer\n",
      "optimizer = optimizers.Adam()\n",
      "optimizer.setup(model)\n",
      "\n",
      "# Init/Resume\n",
      "if args.initmodel:\n",
      "    print('Load model from', args.initmodel)\n",
      "    serializers.load_npz(args.initmodel, model)\n",
      "if args.resume:\n",
      "    print('Load optimizer state from', args.resume)\n",
      "    serializers.load_npz(args.resume, optimizer)\n",
      "\n",
      "# Learning loop\n",
      "for epoch in six.moves.range(1, n_epoch + 1):\n",
      "    print('epoch', epoch)\n",
      "\n",
      "    # training\n",
      "    perm = np.random.permutation(N)\n",
      "    sum_accuracy = 0\n",
      "    sum_loss = 0\n",
      "    start = time.time()\n",
      "    for i in six.moves.range(0, N, batchsize):\n",
      "        x = chainer.Variable(xp.asarray(x_train[perm[i:i + batchsize]]))\n",
      "        t = chainer.Variable(xp.asarray(y_train[perm[i:i + batchsize]]))\n",
      "\n",
      "        # Pass the loss function (Classifier defines it) and its arguments\n",
      "        optimizer.update(model, x, t)\n",
      "\n",
      "        if epoch == 1 and i == 0:\n",
      "            with open('graph.dot', 'w') as o:\n",
      "                variable_style = {'shape': 'octagon', 'fillcolor': '#E0E0E0',\n",
      "                                  'style': 'filled'}\n",
      "                function_style = {'shape': 'record', 'fillcolor': '#6495ED',\n",
      "                                  'style': 'filled'}\n",
      "                g = computational_graph.build_computational_graph(\n",
      "                    (model.loss, ),\n",
      "                    variable_style=variable_style,\n",
      "                    function_style=function_style)\n",
      "                o.write(g.dump())\n",
      "            print('graph generated')\n",
      "\n",
      "        sum_loss += float(model.loss.data) * len(t.data)\n",
      "        sum_accuracy += float(model.accuracy.data) * len(t.data)\n",
      "    end = time.time()\n",
      "    elapsed_time = end - start\n",
      "    throughput = N / elapsed_time\n",
      "    print('train mean loss={}, accuracy={}, throughput={} images/sec'.format(\n",
      "        sum_loss / N, sum_accuracy / N, throughput))\n",
      "\n",
      "    # evaluation\n",
      "    sum_accuracy = 0\n",
      "    sum_loss = 0\n",
      "    for i in six.moves.range(0, N_test, batchsize):\n",
      "        x = chainer.Variable(xp.asarray(x_test[i:i + batchsize]),\n",
      "                             volatile='on')\n",
      "        t = chainer.Variable(xp.asarray(y_test[i:i + batchsize]),\n",
      "                             volatile='on')\n",
      "        loss = model(x, t)\n",
      "        sum_loss += float(loss.data) * len(t.data)\n",
      "        sum_accuracy += float(model.accuracy.data) * len(t.data)\n",
      "\n",
      "    print('test  mean loss={}, accuracy={}'.format(\n",
      "        sum_loss / N_test, sum_accuracy / N_test))\n",
      "\n",
      "# Save the model and the optimizer\n",
      "print('save the model')\n",
      "serializers.save_npz('mlp.model', model)\n",
      "print('save the optimizer')\n",
      "serializers.save_npz('mlp.state', optimizer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'args' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-8-41c959bbf4d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mbatchsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mn_units\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python \n",
      "\"\"\"Chainer example: train a multi-layer perceptron on diabetes dataset \n",
      "  \n",
      "This is a minimal example to write a feed-forward net. It requires scikit-learn \n",
      "to load diabetes dataset. \n",
      "  \n",
      "\"\"\" \n",
      "import argparse \n",
      "import numpy as np \n",
      "from sklearn.datasets import load_diabetes \n",
      "from chainer import cuda, Variable, FunctionSet, optimizers \n",
      "import chainer.functions  as F \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "parser = argparse.ArgumentParser(description='Chainer example: MNIST') \n",
      "parser.add_argument('--gpu', '-g', default=-1, type=int, help='GPU ID (negative value indicates CPU)') \n",
      "args = parser.parse_args() \n",
      "\n",
      " \n",
      "batchsize = 13 \n",
      "n_epoch   = 100 \n",
      "n_units   = 30 \n",
      "\n",
      " \n",
      "# Prepare dataset \n",
      "print('fetch diabetes dataset') \n",
      "diabetes = load_diabetes() \n",
      "data = diabetes['data'].astype(np.float32) \n",
      "target = diabetes['target'].astype(np.float32).reshape(len(diabetes['target']), 1) \n",
      " \n",
      " \n",
      "N = batchsize * 30 \n",
      "x_train, x_test = np.split(data, [N]) \n",
      "y_train, y_test = np.split(target, [N]) \n",
      "N_test = y_test.size \n",
      " \n",
      " \n",
      "# Prepare multi-layer perceptron model \n",
      "model = FunctionSet(l1=F.Linear(10, n_units), \n",
      "                    l2=F.Linear(n_units, n_units), \n",
      "                    l3=F.Linear(n_units, 1)) \n",
      "if args.gpu >= 0: \n",
      "    cuda.init(args.gpu) \n",
      "    model.to_gpu() \n",
      "\n",
      " \n",
      "# Neural net architecture \n",
      "def forward(x_data, y_data, train=True): \n",
      "    x, t = Variable(x_data), Variable(y_data) \n",
      "    h1 = F.dropout(F.relu(model.l1(x)),  train=train) \n",
      "    h2 = F.dropout(F.relu(model.l2(h1)), train=train) \n",
      "    y  = model.l3(h2) \n",
      "    return F.mean_squared_error(y, t), y \n",
      "\n",
      " \n",
      "# Setup optimizer \n",
      "optimizer = optimizers.AdaDelta(rho=0.9) \n",
      "optimizer.setup(model.collect_parameters()) \n",
      "\n",
      "# Learning loop \n",
      "for epoch in xrange(1, n_epoch+1): \n",
      "    print('epoch', epoch) \n",
      " \n",
      "\n",
      "    # training \n",
      "    perm = np.random.permutation(N) \n",
      "    sum_loss = 0 \n",
      "\n",
      " \n",
      "    for i in xrange(0, N, batchsize): \n",
      "        x_batch = x_train[perm[i:i+batchsize]] \n",
      "        y_batch = y_train[perm[i:i+batchsize]] \n",
      "        if args.gpu >= 0: \n",
      "            x_batch = cuda.to_gpu(x_batch) \n",
      "            y_batch = cuda.to_gpu(y_batch) \n",
      "\n",
      " \n",
      "        optimizer.zero_grads() \n",
      "        loss, pred = forward(x_batch, y_batch) \n",
      "        loss.backward() \n",
      "        optimizer.update() \n",
      "\n",
      " \n",
      "        sum_loss += float(cuda.to_cpu(loss.data)) * batchsize \n",
      "\n",
      " \n",
      "    print('train mean loss={}').format( sum_loss / N) \n",
      "\n",
      " \n",
      "    sum_loss     = 0 \n",
      "    preds = [] \n",
      "    for i in xrange(0, N_test, batchsize): \n",
      "        x_batch = x_test[i:i+batchsize] \n",
      "        y_batch = y_test[i:i+batchsize] \n",
      "        if args.gpu >= 0: \n",
      "            x_batch = cuda.to_gpu(x_batch) \n",
      "            y_batch = cuda.to_gpu(y_batch) \n",
      "\n",
      " \n",
      "        loss, pred = forward(x_batch, y_batch, train=False) \n",
      "        preds.extend(cuda.to_cpu(pred.data)) \n",
      "        sum_loss     += float(cuda.to_cpu(loss.data)) * batchsize \n",
      "    pearson = np.corrcoef(np.asarray(preds).reshape(len(preds),), np.asarray(y_test).reshape(len(preds),)) \n",
      "     \n",
      " \n",
      " \n",
      "    print('test  mean loss={}, corrcoef={}').format(sum_loss / N_test, pearson[0][1]) \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "usage: -c [-h] [--gpu GPU]\n",
        "-c: error: unrecognized arguments: -f C:\\Users\\Kensuke\\.ipython\\profile_default\\security\\kernel-2fdca8a2-d8c4-49a9-a2b4-048ec80050ac.json --IPKernelApp.parent_appname='ipython-notebook' --profile-dir C:\\Users\\Kensuke\\.ipython\\profile_default --interrupt=1632 --parent=1652\n"
       ]
      },
      {
       "ename": "SystemExit",
       "evalue": "2",
       "output_type": "pyerr",
       "traceback": [
        "An exception has occurred, use %tb to see the full traceback.\n",
        "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "To exit: use 'exit', 'quit', or Ctrl-D.\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}